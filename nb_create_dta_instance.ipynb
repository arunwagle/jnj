{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create DTA Instance\n",
        "\n",
        "Creates a new DTA entity in the DTA table.\n",
        "\n",
        "**IMPORTANT:** A DTA is library-type agnostic - it's a container for ALL library types\n",
        "(transfer_variables, test_concepts, codelists, etc.). The downstream notebooks\n",
        "(save_draft, approve_dta) iterate over ALL configured library types.\n",
        "\n",
        "**Sources:**\n",
        "- `HISTORICAL` - From historical tsDTA processing (workflow_state=APPROVED)\n",
        "- `UI` - From UI-initiated DTA creation (workflow_state=NOT_STARTED)\n",
        "\n",
        "**What it does:**\n",
        "1. Reads distinct trial metadata from primary silver table\n",
        "2. Auto-generates DTA ID as UUID\n",
        "3. Checks for MANUAL_REVIEW_REQUIRED records\n",
        "4. Sets DTA status = MANUAL_REVIEW or ACTIVE accordingly\n",
        "5. Creates DTA entity with audit columns\n",
        "6. Creates DTA_WORKFLOW record for historical (auto-approved)\n",
        "7. Creates DTA_APPROVAL_TASK records:\n",
        "   - HISTORICAL: Single SYSTEM_APPROVED task (auto-approved) - represents automatic DTA Major creation\n",
        "   - UI: JNJ_DAE + VENDOR tasks (PENDING) - manual approval required\n",
        "8. Sets task values for downstream processing (source, dta_ids, library_type=\"ALL\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Imports\n",
        "import json\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, BooleanType\n",
        "from clinical_data_standards_framework.utils import save_with_audit\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"CREATE DTA INSTANCE\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Get Configuration from Setup Task\n",
        "globals_dict = json.loads(dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"globals\"))\n",
        "versioning_dict = json.loads(dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"versioning\"))\n",
        "created_by_principal = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"created_by_principal\")\n",
        "databricks_job_id = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"databricks_job_id\")\n",
        "databricks_job_name = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"databricks_job_name\")\n",
        "databricks_run_id = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"databricks_run_id\")\n",
        "\n",
        "catalog = globals_dict['catalog']\n",
        "silver_schema = globals_dict['silver_schema']\n",
        "gold_schema = globals_dict['gold_schema']\n",
        "\n",
        "# Versioning configuration\n",
        "versioning_enabled = versioning_dict.get('enabled', False)\n",
        "registry_table_name = versioning_dict.get('registry_table', 'md_version_registry')\n",
        "library_tables = versioning_dict.get('library_tables', [])\n",
        "\n",
        "# Validate versioning config - must have library tables configured\n",
        "if not library_tables:\n",
        "    raise ValueError(\n",
        "        \"No library_tables configured in versioning config. \"\n",
        "        \"Check that md_config_cache is populated correctly with job_populate_config_cache.\"\n",
        "    )\n",
        "\n",
        "print(f\"Catalog: {catalog}\")\n",
        "print(f\"Silver Schema: {silver_schema}\")\n",
        "print(f\"Gold Schema: {gold_schema}\")\n",
        "print(f\"Versioning Enabled: {versioning_enabled}\")\n",
        "print(f\"Library Types: {[lib.get('library_type') for lib in library_tables]}\")\n",
        "print(f\"Created by: {created_by_principal}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Get Parameters from Job (passed via base_parameters)\n",
        "# NOTE: DTA creation is library-type agnostic - one DTA contains ALL library types\n",
        "# The downstream notebooks (save_draft, approve_dta) iterate over ALL library types\n",
        "\n",
        "try:\n",
        "    source = dbutils.widgets.get(\"source\") or \"HISTORICAL\"\n",
        "    trial_id_param = dbutils.widgets.get(\"trial_id\")\n",
        "    stream_type_param = dbutils.widgets.get(\"data_stream_type\")\n",
        "    provider_param = dbutils.widgets.get(\"data_provider_name\")\n",
        "except Exception as e:\n",
        "    raise ValueError(\n",
        "        f\"Could not get job parameters: {e}. \"\n",
        "        \"This notebook must be run as part of job_cdm_dta_create with proper parameters.\"\n",
        "    )\n",
        "\n",
        "print(f\"\\nSource: {source}\")\n",
        "print(f\"Trial ID filter: {trial_id_param or '(all from data)'}\")\n",
        "print(f\"Stream Type filter: {stream_type_param or '(all from data)'}\")\n",
        "print(f\"Provider filter: {provider_param or '(all from data)'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Define Table Schemas\n",
        "\n",
        "def get_dta_schema():\n",
        "    \"\"\"\n",
        "    =============================================================================\n",
        "    SCHEMA: dta (Gold Layer - Data Transfer Agreement)\n",
        "    =============================================================================\n",
        "    This table stores the core metadata for each Trial-Specific Data Transfer \n",
        "    Agreement (tsDTA). Each record represents a unique data transfer contract \n",
        "    between J&J and a data provider for a specific trial and data stream.\n",
        "    \n",
        "    The table includes identifiers for the trial and data stream, references to \n",
        "    the parent document, the provider, the transfer type, and the current status \n",
        "    of the DTA (e.g., Draft, Review, Approved).\n",
        "    \n",
        "    Version metadata and audit timestamps support governance and traceability.\n",
        "    This table is the authoritative record of all active and historical DTAs \n",
        "    used to drive ingestion rules and metadata validation.\n",
        "    \n",
        "    Version Pointers:\n",
        "        - version: Active version (draft or major) - what UI displays\n",
        "        - current_draft_version: Latest draft in Silver (e.g., \"1.0-DTA001-draft3\")\n",
        "        - base_template_version: Library version branched from (e.g., \"1.0\")\n",
        "    \n",
        "    Note: latest_major_version was removed - now derived from md_version_registry\n",
        "    via: SELECT version FROM md_version_registry WHERE dta_id = :id AND version_type = 'DTA_APPROVED' AND status = 'ACTIVE'\n",
        "    \n",
        "    Status Alignment (1:1 with md_version_registry.status):\n",
        "        - DRAFT: Being edited (effective_end_ts = NULL)\n",
        "        - ACTIVE: Approved and in use (effective_end_ts = NULL)\n",
        "        - PROMOTED: Elevated to template (effective_end_ts = NULL)\n",
        "        - ARCHIVED: No longer active (effective_end_ts = timestamp)\n",
        "    \n",
        "    DTA Name Generation:\n",
        "        - Historical import: {trial_id}_{data_stream_type}_{data_provider_name}\n",
        "        - UI clone: <Change_this>_{trial_id}_{data_stream_type}_{data_provider_name}\n",
        "    =============================================================================\n",
        "    \"\"\"\n",
        "    return StructType([\n",
        "        # Primary & Business Keys\n",
        "        StructField(\"dta_id\", StringType(), False),  # UUID primary key\n",
        "        StructField(\"dta_number\", StringType(), False),  # Sequential ID (DTA001, DTA002) for display\n",
        "        StructField(\"dta_name\", StringType(), True),  # User-friendly name for UI\n",
        "        # Lineage\n",
        "        StructField(\"parent_document_id\", StringType(), True),  # FK to source tsDTA document\n",
        "        # Business Key: trial_id + data_stream_type + data_provider_name\n",
        "        StructField(\"trial_id\", StringType(), True),  # Clinical trial ID (e.g., VAC18193RSV3001)\n",
        "        StructField(\"data_stream_type\", StringType(), True),  # Data category (Labs, Adverse Events)\n",
        "        StructField(\"data_provider_name\", StringType(), True),  # Vendor (LabCorp, Covance, PPD)\n",
        "        # Status & Workflow (status aligns 1:1 with md_version_registry.status)\n",
        "        StructField(\"status\", StringType(), False),  # DRAFT, ACTIVE, PROMOTED, ARCHIVED\n",
        "        StructField(\"workflow_state\", StringType(), False),  # NOT_STARTED, IN_REVIEW, APPROVED, REJECTED\n",
        "        StructField(\"workflow_iteration\", IntegerType(), True),  # Current approval cycle (1, 2, 3...)\n",
        "        # Version Pointers (latest_major_version removed - derived from md_version_registry)\n",
        "        StructField(\"version\", StringType(), True),  # Active version for UI display\n",
        "        StructField(\"current_draft_version\", StringType(), True),  # Latest draft in Silver\n",
        "        StructField(\"base_template_version\", StringType(), True),  # Branched from library version\n",
        "        StructField(\"notes\", StringType(), True),  # General notes about the DTA\n",
        "        # Audit columns\n",
        "        StructField(\"created_by_principal\", StringType(), True),  # User who created\n",
        "        StructField(\"created_ts\", TimestampType(), True),  # Creation timestamp\n",
        "        StructField(\"last_updated_by_principal\", StringType(), True),  # User who last modified\n",
        "        StructField(\"last_updated_ts\", TimestampType(), True),  # Last modification timestamp\n",
        "        StructField(\"databricks_job_id\", StringType(), True),  # Job ID for lineage\n",
        "        StructField(\"databricks_job_name\", StringType(), True),  # Job name for lineage\n",
        "        StructField(\"databricks_run_id\", StringType(), True)  # Run ID for lineage\n",
        "    ])\n",
        "\n",
        "def get_dta_workflow_schema():\n",
        "    \"\"\"\n",
        "    =============================================================================\n",
        "    SCHEMA: dta_workflow (Gold Layer - Workflow Tracking)\n",
        "    =============================================================================\n",
        "    This table tracks the approval lifecycle for DTAs across all stakeholders \n",
        "    involved in the metadata governance process.\n",
        "    \n",
        "    It models workflow steps between J&J reviewers, external data providers \n",
        "    (vendors), and J&J librarians, who are responsible for promoting finalized \n",
        "    metadata into the official library versions.\n",
        "    \n",
        "    The table captures workflow identifiers, statuses, and audit timestamps for \n",
        "    initiation and updates. It provides the structured sequence of approval \n",
        "    events required to move a DTA from Draft to Review to Approved to Published \n",
        "    Metadata Library.\n",
        "    =============================================================================\n",
        "    \"\"\"\n",
        "    return StructType([\n",
        "        StructField(\"dta_workflow_id\", StringType(), False),  # UUID primary key\n",
        "        StructField(\"dta_id\", StringType(), False),  # FK to dta table\n",
        "        StructField(\"workflow_iteration\", IntegerType(), False),  # Cycle number (1, 2, 3...)\n",
        "        StructField(\"workflow_status\", StringType(), False),  # NOT_STARTED, IN_REVIEW, APPROVED, REJECTED\n",
        "        StructField(\"summary_comment\", StringType(), True),  # Workflow notes/rejection reasons\n",
        "        StructField(\"initiated_ts\", TimestampType(), True),  # When workflow cycle started\n",
        "        StructField(\"closed_ts\", TimestampType(), True),  # When workflow cycle completed\n",
        "        # Audit columns\n",
        "        StructField(\"created_by_principal\", StringType(), True),\n",
        "        StructField(\"created_ts\", TimestampType(), True),\n",
        "        StructField(\"last_updated_by_principal\", StringType(), True),\n",
        "        StructField(\"last_updated_ts\", TimestampType(), True),\n",
        "        StructField(\"databricks_job_id\", StringType(), True),\n",
        "        StructField(\"databricks_job_name\", StringType(), True),\n",
        "        StructField(\"databricks_run_id\", StringType(), True)\n",
        "    ])\n",
        "\n",
        "def get_dta_approval_task_schema():\n",
        "    \"\"\"\n",
        "    =============================================================================\n",
        "    SCHEMA: dta_approval_task (Gold Layer - Approval Tasks)\n",
        "    =============================================================================\n",
        "    Individual approval tasks within a DTA workflow.\n",
        "    \n",
        "    HISTORICAL imports:\n",
        "        - Single SYSTEM_APPROVED task (is_auto_approve=True, status=APPROVED)\n",
        "        - Represents automatic DTA Major creation for pre-approved historical data\n",
        "        - No manual approval needed since data is already validated\n",
        "    \n",
        "    UI flows:\n",
        "        - JNJ_DAE task (manual approval, approval_order=1)\n",
        "        - VENDOR task (manual approval, approval_order=2)\n",
        "        - When BOTH approve, DTA Major is created by the approval API\n",
        "        - No SYSTEM_APPROVED task for UI - promotion is triggered by last approver\n",
        "    \n",
        "    NOTE: Audit columns are added automatically by save_with_audit()\n",
        "    =============================================================================\n",
        "    \"\"\"\n",
        "    return StructType([\n",
        "        StructField(\"approval_task_id\", StringType(), False),      # UUID PK\n",
        "        StructField(\"dta_workflow_id\", StringType(), False),       # FK to dta_workflow\n",
        "        StructField(\"dta_id\", StringType(), False),                # FK to dta (denormalized)\n",
        "        StructField(\"approver_role\", StringType(), False),         # JNJ_DAE, VENDOR, SYSTEM_APPROVED\n",
        "        StructField(\"assigned_to_principal\", StringType(), True),  # User email (NULL for SYSTEM)\n",
        "        StructField(\"approval_status\", StringType(), False),       # PENDING, APPROVED, REJECTED\n",
        "        StructField(\"approval_order\", IntegerType(), False),       # Sequence (1, 2, 99)\n",
        "        StructField(\"is_auto_approve\", BooleanType(), True),       # True for SYSTEM_APPROVED\n",
        "        StructField(\"approval_comment\", StringType(), True),       # Comment from approver\n",
        "        StructField(\"approved_ts\", TimestampType(), True)          # When approved/rejected\n",
        "        # NOTE: Audit columns added automatically by save_with_audit()\n",
        "    ])\n",
        "\n",
        "print(\"Schemas defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Configure Table Names and Find Existing Silver Tables\n",
        "# DTA entity is library-type agnostic - it's a container for ALL library types\n",
        "# We find ALL existing silver tables and will UNION them to get DTA contexts\n",
        "\n",
        "# DTA tables (Gold layer)\n",
        "dta_table = f\"{catalog}.{gold_schema}.dta\"\n",
        "dta_workflow_table = f\"{catalog}.{gold_schema}.dta_workflow\"\n",
        "dta_approval_task_table = f\"{catalog}.{gold_schema}.dta_approval_task\"\n",
        "\n",
        "# Reference tables for normalized values (token overlap matching)\n",
        "vendor_table = f\"{catalog}.{gold_schema}.md_vendor\"\n",
        "data_stream_table = f\"{catalog}.{gold_schema}.md_data_stream\"\n",
        "\n",
        "# Registry table from config\n",
        "registry_table = f\"{catalog}.{gold_schema}.{registry_table_name}\"\n",
        "\n",
        "# Find ALL existing silver tables with data\n",
        "existing_silver_tables = []\n",
        "print(\"Searching for existing silver tables with data...\")\n",
        "for lib in library_tables:\n",
        "    table_name = lib.get('silver_table')\n",
        "    table_schema = lib.get('silver_schema', silver_schema)\n",
        "    full_table = f\"{catalog}.{table_schema}.{table_name}\"\n",
        "    \n",
        "    if spark.catalog.tableExists(full_table):\n",
        "        count = spark.table(full_table).count()\n",
        "        if count > 0:\n",
        "            existing_silver_tables.append({\n",
        "                'table': full_table,\n",
        "                'library_type': lib.get('library_type'),\n",
        "                'count': count\n",
        "            })\n",
        "            print(f\"  âœ“ Found {lib.get('library_type')}: {full_table} ({count} rows)\")\n",
        "        else:\n",
        "            print(f\"  â­ Skipping {lib.get('library_type')}: table exists but empty\")\n",
        "    else:\n",
        "        print(f\"  â­ Skipping {lib.get('library_type')}: table does not exist\")\n",
        "\n",
        "if not existing_silver_tables:\n",
        "    raise ValueError(\n",
        "        f\"No silver tables with data found. Available library tables: \"\n",
        "        f\"{[lib.get('silver_table') for lib in library_tables]}. \"\n",
        "        f\"Run at least one processor (tsDTA, OA, etc.) before creating DTA.\"\n",
        "    )\n",
        "\n",
        "print(f\"\\nâœ“ Found {len(existing_silver_tables)} silver table(s) with data\")\n",
        "print(f\"DTA Table: {dta_table}\")\n",
        "print(f\"DTA Workflow Table: {dta_workflow_table}\")\n",
        "print(f\"DTA Approval Task Table: {dta_approval_task_table}\")\n",
        "print(f\"Registry Table: {registry_table}\")\n",
        "print(f\"\\nAll Library Tables (from config):\")\n",
        "for lib in library_tables:\n",
        "    lib_table = f\"{catalog}.{lib.get('schema', gold_schema)}.{lib.get('name')}\"\n",
        "    print(f\"  {lib.get('library_type')}: {lib_table}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Read Silver Data and Get Distinct DTAs\n",
        "# UNION all existing silver tables to get DTA contexts\n",
        "# One parent_document_id = One DTA (parent_document_id is the source file, e.g., ZIP file)\n",
        "# Uses NORMALIZED vendor/data_stream names from reference tables (token overlap matching)\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"Reading silver data for DTA metadata...\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "# Union all existing silver tables to get DTA contexts\n",
        "df_union = None\n",
        "for silver_info in existing_silver_tables:\n",
        "    df_silver = spark.table(silver_info['table'])\n",
        "    \n",
        "    # Select common context columns - parent_document_id is ALWAYS used\n",
        "    df_context = df_silver.select(\n",
        "        \"parent_document_id\",\n",
        "        \"trial_id\", \n",
        "        \"data_stream_type\", \n",
        "        \"data_provider_name\"\n",
        "    )\n",
        "    \n",
        "    if df_union is None:\n",
        "        df_union = df_context\n",
        "    else:\n",
        "        df_union = df_union.unionByName(df_context)\n",
        "    \n",
        "    print(f\"  Added contexts from {silver_info['library_type']}\")\n",
        "\n",
        "# ============================================================================\n",
        "# NORMALIZATION: Join with reference tables to get canonical vendor/data_stream names\n",
        "# This ensures that variant names (e.g., \"Labcorp CLS\" and \"Labcorp\") map to the same DTA\n",
        "# ============================================================================\n",
        "print(\"\\n  Applying vendor/data_stream normalization...\")\n",
        "\n",
        "# Load reference tables if they exist\n",
        "df_vendors = None\n",
        "df_streams = None\n",
        "\n",
        "if spark.catalog.tableExists(vendor_table):\n",
        "    df_vendors = spark.table(vendor_table).select(\n",
        "        F.col(\"vendor_name\").alias(\"canonical_vendor\"),\n",
        "        F.col(\"aliases\").alias(\"vendor_aliases\")\n",
        "    )\n",
        "    print(f\"    âœ“ Loaded vendor reference table: {df_vendors.count()} vendors\")\n",
        "\n",
        "if spark.catalog.tableExists(data_stream_table):\n",
        "    df_streams = spark.table(data_stream_table).select(\n",
        "        F.col(\"data_stream_name\").alias(\"canonical_stream\"),\n",
        "        F.col(\"aliases\").alias(\"stream_aliases\")\n",
        "    )\n",
        "    print(f\"    âœ“ Loaded data stream reference table: {df_streams.count()} streams\")\n",
        "\n",
        "# Apply normalization using array_contains for alias matching\n",
        "if df_vendors is not None:\n",
        "    # Explode aliases and join to find canonical vendor name\n",
        "    df_union = df_union.join(\n",
        "        df_vendors,\n",
        "        F.array_contains(F.col(\"vendor_aliases\"), F.col(\"data_provider_name\")) |\n",
        "        (F.col(\"canonical_vendor\") == F.col(\"data_provider_name\")),\n",
        "        \"left\"\n",
        "    ).select(\n",
        "        \"parent_document_id\",\n",
        "        \"trial_id\",\n",
        "        \"data_stream_type\",\n",
        "        F.coalesce(F.col(\"canonical_vendor\"), F.col(\"data_provider_name\")).alias(\"data_provider_name\")\n",
        "    )\n",
        "    print(f\"    âœ“ Applied vendor normalization\")\n",
        "\n",
        "if df_streams is not None:\n",
        "    # Join to find canonical stream name\n",
        "    df_union = df_union.join(\n",
        "        df_streams,\n",
        "        F.array_contains(F.col(\"stream_aliases\"), F.col(\"data_stream_type\")) |\n",
        "        (F.col(\"canonical_stream\") == F.col(\"data_stream_type\")),\n",
        "        \"left\"\n",
        "    ).select(\n",
        "        \"parent_document_id\",\n",
        "        \"trial_id\",\n",
        "        F.coalesce(F.col(\"canonical_stream\"), F.col(\"data_stream_type\")).alias(\"data_stream_type\"),\n",
        "        \"data_provider_name\"\n",
        "    )\n",
        "    print(f\"    âœ“ Applied data stream normalization\")\n",
        "\n",
        "# ============================================================================\n",
        "# GROUP BY parent_document_id to ensure ONE DTA per source file\n",
        "# Take the first non-null values for trial_id, data_stream_type, data_provider_name\n",
        "# This prevents duplicate DTAs when tsDTA and OA have slightly different values\n",
        "# ============================================================================\n",
        "print(\"\\n  Grouping by parent_document_id (one DTA per source file)...\")\n",
        "\n",
        "df_dta_contexts = df_union.groupBy(\"parent_document_id\").agg(\n",
        "    F.first(\"trial_id\", ignorenulls=True).alias(\"trial_id\"),\n",
        "    F.first(\"data_stream_type\", ignorenulls=True).alias(\"data_stream_type\"),\n",
        "    F.first(\"data_provider_name\", ignorenulls=True).alias(\"data_provider_name\")\n",
        ")\n",
        "\n",
        "print(f\"  âœ“ Grouped to {df_dta_contexts.count()} unique parent_document_id(s)\")\n",
        "\n",
        "# If parameters provided, filter to specific context\n",
        "if trial_id_param:\n",
        "    df_dta_contexts = df_dta_contexts.filter(F.col(\"trial_id\") == trial_id_param)\n",
        "if stream_type_param:\n",
        "    df_dta_contexts = df_dta_contexts.filter(F.col(\"data_stream_type\") == stream_type_param)\n",
        "if provider_param:\n",
        "    df_dta_contexts = df_dta_contexts.filter(F.col(\"data_provider_name\") == provider_param)\n",
        "\n",
        "dta_contexts = df_dta_contexts.collect()\n",
        "print(f\"Found {len(dta_contexts)} DTA context(s) to process\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Ensure Tables Exist\n",
        "now = datetime.now()\n",
        "\n",
        "# Create DTA table if not exists\n",
        "if not spark.catalog.tableExists(dta_table):\n",
        "    print(f\"Creating DTA table: {dta_table}\")\n",
        "    empty_df = spark.createDataFrame([], schema=get_dta_schema())\n",
        "    empty_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(dta_table)\n",
        "    print(f\"Created {dta_table}\")\n",
        "else:\n",
        "    print(f\"DTA table exists: {dta_table}\")\n",
        "\n",
        "# Create DTA_WORKFLOW table if not exists\n",
        "if not spark.catalog.tableExists(dta_workflow_table):\n",
        "    print(f\"Creating DTA_WORKFLOW table: {dta_workflow_table}\")\n",
        "    empty_df = spark.createDataFrame([], schema=get_dta_workflow_schema())\n",
        "    empty_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(dta_workflow_table)\n",
        "    print(f\"Created {dta_workflow_table}\")\n",
        "else:\n",
        "    print(f\"DTA_WORKFLOW table exists: {dta_workflow_table}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Create DTA Instances\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"Creating DTA instances...\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "# Get the current max DTA number from existing table\n",
        "def get_next_dta_number(spark, dta_table, offset=0):\n",
        "    \"\"\"Get the next sequential DTA number (DTA001, DTA002, etc.)\"\"\"\n",
        "    try:\n",
        "        # Check if table exists first\n",
        "        if not spark.catalog.tableExists(dta_table):\n",
        "            print(f\"  âš ï¸ DTA table does not exist yet, starting from DTA001\")\n",
        "            return f\"DTA{(1 + offset):03d}\"\n",
        "        \n",
        "        # Query for max number\n",
        "        result = spark.sql(f\"\"\"\n",
        "            SELECT MAX(CAST(REGEXP_EXTRACT(dta_number, 'DTA([0-9]+)', 1) AS INT)) as max_num\n",
        "            FROM {dta_table}\n",
        "        \"\"\").first()\n",
        "        \n",
        "        max_num = result.max_num if result and result.max_num is not None else 0\n",
        "        next_num = max_num + 1 + offset\n",
        "        \n",
        "        print(f\"  ðŸ“Š Current max DTA number: {max_num}, Next: DTA{next_num:03d}\")\n",
        "        return f\"DTA{next_num:03d}\"\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  âŒ ERROR getting DTA number: {e}\")\n",
        "        print(f\"  âš ï¸ Defaulting to DTA{(1 + offset):03d}\")\n",
        "        return f\"DTA{(1 + offset):03d}\"\n",
        "\n",
        "dta_records = []\n",
        "workflow_records = []\n",
        "approval_task_records = []  # Approval tasks for workflow\n",
        "created_dta_ids = []\n",
        "created_dta_numbers = []  # Track DTA numbers for version tag\n",
        "\n",
        "for idx, ctx in enumerate(dta_contexts):\n",
        "    parent_document_id = ctx[\"parent_document_id\"]  # Always present (source file ID)\n",
        "    trial_id = ctx[\"trial_id\"]\n",
        "    stream_type = ctx[\"data_stream_type\"]\n",
        "    provider = ctx[\"data_provider_name\"]\n",
        "    \n",
        "    print(f\"\\nProcessing: {trial_id} / {stream_type} / {provider} (Doc: {parent_document_id[:8]}...)\")\n",
        "    \n",
        "    # Build filter condition - always filter by parent_document_id (one DTA per source file)\n",
        "    filter_condition = (F.col(\"parent_document_id\") == parent_document_id)\n",
        "    \n",
        "    # Check for MANUAL_REVIEW_REQUIRED records across all silver tables\n",
        "    review_count = 0\n",
        "    total_count = 0\n",
        "    for silver_info in existing_silver_tables:\n",
        "        df_silver = spark.table(silver_info['table'])\n",
        "        # Check if status column exists in this table\n",
        "        if \"status\" in df_silver.columns:\n",
        "            review_count += df_silver.filter(\n",
        "                filter_condition &\n",
        "                (F.col(\"status\") == \"MANUAL_REVIEW_REQUIRED\")\n",
        "            ).count()\n",
        "        total_count += df_silver.filter(filter_condition).count()\n",
        "    \n",
        "    needs_review = review_count > 0\n",
        "    print(f\"  Total records: {total_count}, Needs review: {review_count}\")\n",
        "    \n",
        "    # Generate UUID for DTA\n",
        "    dta_id = str(uuid.uuid4())\n",
        "    \n",
        "    # Generate sequential DTA number (DTA001, DTA002, etc.)\n",
        "    dta_number = get_next_dta_number(spark, dta_table, offset=idx)\n",
        "    \n",
        "    created_dta_ids.append(dta_id)\n",
        "    created_dta_numbers.append(dta_number)\n",
        "    \n",
        "    # Determine status, workflow_state, and version pointers based on source\n",
        "    # Base library version - what this DTA branches from\n",
        "    base_template_version = \"1.0\"  # TODO: Could query latest DTA_TEMPLATE from registry\n",
        "    \n",
        "    if source == \"HISTORICAL\":\n",
        "        # Historical: Already approved, will go directly to DTA_APPROVED\n",
        "        status = \"MANUAL_REVIEW\" if needs_review else \"ACTIVE\"\n",
        "        workflow_state = \"APPROVED\"\n",
        "        notes = f\"Created from historical tsDTA data. Records needing review: {review_count}/{total_count}\"\n",
        "        # Version pointers for HISTORICAL (will be set after DTA_APPROVED creation)\n",
        "        current_draft_version = None  # No draft phase for historical\n",
        "        version = None  # Will be set by create_historical_dta_major\n",
        "    else:  # UI\n",
        "        # UI: Starts as draft in Silver\n",
        "        status = \"DRAFT\"\n",
        "        workflow_state = \"NOT_STARTED\"\n",
        "        notes = \"Created from UI\"\n",
        "        # Version pointers for UI (draft starts in Silver)\n",
        "        draft_version = f\"{base_template_version}-{dta_number}-draft1\"\n",
        "        current_draft_version = draft_version\n",
        "        version = draft_version  # Points to Silver draft\n",
        "    \n",
        "    # Create DTA record\n",
        "    dta_record = {\n",
        "        \"dta_id\": dta_id,\n",
        "        \"dta_number\": dta_number,  # Sequential DTA number for version tags\n",
        "        \"parent_document_id\": parent_document_id,  # Link to source document (for historical lineage)\n",
        "        \"trial_id\": trial_id,\n",
        "        \"data_stream_type\": stream_type,\n",
        "        \"data_provider_name\": provider,\n",
        "        \"status\": status,\n",
        "        \"workflow_state\": workflow_state,\n",
        "        \"workflow_iteration\": 1 if source == \"HISTORICAL\" else 0,\n",
        "        # Version Pointers (latest_major_version removed - derived from md_version_registry)\n",
        "        \"version\": version,\n",
        "        \"current_draft_version\": current_draft_version,\n",
        "        \"base_template_version\": base_template_version,\n",
        "        \"notes\": notes,\n",
        "        # SCD Type 2 columns\n",
        "        \"effective_start_ts\": now,  # When this DTA became active\n",
        "        \"effective_end_ts\": None,   # NULL means currently active\n",
        "        \"is_current\": True,         # Quick filter for active records\n",
        "        # Audit columns\n",
        "        \"created_by_principal\": created_by_principal,\n",
        "        \"created_ts\": now,\n",
        "        \"last_updated_by_principal\": created_by_principal,\n",
        "        \"last_updated_ts\": now,\n",
        "        \"databricks_job_id\": databricks_job_id,\n",
        "        \"databricks_job_name\": databricks_job_name,\n",
        "        \"databricks_run_id\": databricks_run_id\n",
        "    }\n",
        "    dta_records.append(dta_record)\n",
        "    \n",
        "    # For HISTORICAL, create auto-approved workflow record and approval task\n",
        "    if source == \"HISTORICAL\":\n",
        "        workflow_id = str(uuid.uuid4())\n",
        "        \n",
        "        # Create workflow record (auto-approved for historical)\n",
        "        workflow_record = {\n",
        "            \"dta_workflow_id\": workflow_id,\n",
        "            \"dta_id\": dta_id,\n",
        "            \"workflow_iteration\": 1,\n",
        "            \"workflow_status\": \"APPROVED\",\n",
        "            \"summary_comment\": \"Historical DTA - auto-approved during import\",\n",
        "            \"initiated_ts\": now,\n",
        "            \"closed_ts\": now,\n",
        "            \"created_by_principal\": created_by_principal,\n",
        "            \"created_ts\": now,\n",
        "            \"last_updated_by_principal\": created_by_principal,\n",
        "            \"last_updated_ts\": now,\n",
        "            \"databricks_job_id\": databricks_job_id,\n",
        "            \"databricks_job_name\": databricks_job_name,\n",
        "            \"databricks_run_id\": databricks_run_id\n",
        "        }\n",
        "        workflow_records.append(workflow_record)\n",
        "        \n",
        "        # Create SYSTEM_APPROVED approval task (auto-approved for historical)\n",
        "        # This represents the automatic DTA Major creation for pre-approved historical data\n",
        "        # No JNJ_DAE/VENDOR tasks needed - those are only for UI manual approval flow\n",
        "        approval_task_records.append({\n",
        "            \"approval_task_id\": str(uuid.uuid4()),\n",
        "            \"dta_workflow_id\": workflow_id,\n",
        "            \"dta_id\": dta_id,\n",
        "            \"approver_role\": \"SYSTEM_APPROVED\",\n",
        "            \"assigned_to_principal\": None,  # System task, no user assigned\n",
        "            \"approval_status\": \"APPROVED\",\n",
        "            \"approval_order\": 99,  # Always last in sequence\n",
        "            \"is_auto_approve\": True,\n",
        "            \"approval_comment\": \"Historical import - auto-promoted to DTA Major\",\n",
        "            \"approved_ts\": now\n",
        "        })\n",
        "    \n",
        "    # Generate user-friendly DTA name\n",
        "    # Historical: {trial_id}_{data_stream_type}_{data_provider_name}\n",
        "    # UI: <Change_this>_{trial_id}_{data_stream_type}_{data_provider_name}\n",
        "    if source == \"HISTORICAL\":\n",
        "        dta_name = f\"{trial_id}_{stream_type}_{provider}\"\n",
        "    else:\n",
        "        dta_name = f\"<Change_this>_{trial_id}_{stream_type}_{provider}\"\n",
        "    \n",
        "    # Add dta_name to the record\n",
        "    dta_record[\"dta_name\"] = dta_name\n",
        "    \n",
        "    print(f\"  Created DTA: {dta_number} (ID: {dta_id[:8]}...)\")\n",
        "    print(f\"  Name: {dta_name}\")\n",
        "    print(f\"  Status: {status}, Workflow State: {workflow_state}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: Save to Tables\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"Saving DTA records...\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "if dta_records:\n",
        "    # Save DTA records (with schema evolution for new columns like parent_document_id)\n",
        "    dta_df = spark.createDataFrame(dta_records, schema=get_dta_schema())\n",
        "    dta_df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(dta_table)\n",
        "    print(f\"Saved {len(dta_records)} DTA record(s)\")\n",
        "\n",
        "if workflow_records:\n",
        "    # Save workflow records\n",
        "    wf_df = spark.createDataFrame(workflow_records, schema=get_dta_workflow_schema())\n",
        "    wf_df.write.format(\"delta\").mode(\"append\").saveAsTable(dta_workflow_table)\n",
        "    print(f\"Saved {len(workflow_records)} workflow record(s)\")\n",
        "\n",
        "if approval_task_records:\n",
        "    # Save approval task records using common audit framework\n",
        "    # Audit columns (created_by_principal, created_ts, etc.) are added automatically\n",
        "    task_df = spark.createDataFrame(approval_task_records, schema=get_dta_approval_task_schema())\n",
        "    save_with_audit(\n",
        "        df=task_df,\n",
        "        table_name=dta_approval_task_table,\n",
        "        created_by_principal=created_by_principal,\n",
        "        databricks_job_id=databricks_job_id,\n",
        "        databricks_job_name=databricks_job_name,\n",
        "        databricks_run_id=databricks_run_id,\n",
        "        mode=\"append\"\n",
        "    )\n",
        "    print(f\"Saved {len(approval_task_records)} approval task record(s) (SYSTEM_APPROVED auto-approved)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 10: Set Task Values for Downstream Tasks\n",
        "# Store created DTA IDs and numbers for downstream processing\n",
        "# NOTE: library_type is EMPTY to indicate ALL types should be processed\n",
        "dbutils.jobs.taskValues.set(key=\"created_dta_ids\", value=json.dumps(created_dta_ids))\n",
        "dbutils.jobs.taskValues.set(key=\"created_dta_numbers\", value=json.dumps(created_dta_numbers))\n",
        "dbutils.jobs.taskValues.set(key=\"source\", value=source)\n",
        "dbutils.jobs.taskValues.set(key=\"library_type\", value=\"\")  # Empty = ALL library types\n",
        "dbutils.jobs.taskValues.set(key=\"registry_table\", value=registry_table)\n",
        "dbutils.jobs.taskValues.set(key=\"dta_count\", value=len(created_dta_ids))\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"Summary\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"Source: {source}\")\n",
        "print(f\"Library Types: ALL ({[lib.get('library_type') for lib in library_tables]})\")\n",
        "print(f\"DTAs created: {len(created_dta_ids)}\")\n",
        "for dta_id, dta_num in zip(created_dta_ids, created_dta_numbers):\n",
        "    print(f\"  - {dta_num} (ID: {dta_id[:8]}...)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
