{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Protocol Document Processor (Spark Parallel)\n",
        "\n",
        "Parses Protocol documents using PDF Table of Contents (TOC) for section extraction, with **Spark distributed parallelism** for high performance.\n",
        "\n",
        "## Purpose\n",
        "- Process Protocol documents uploaded via the UI\n",
        "- Use PDF bookmarks/TOC to identify section boundaries (page ranges)\n",
        "- Extract structured data from ALL sections with ordered content\n",
        "- Store in `md_sandbox_documents` for UI preview (Schedule of Activities focus)\n",
        "\n",
        "## Key Features\n",
        "- **Spark Parallel Processing**: Uses `mapInPandas` for distributed section extraction\n",
        "- **TOC-Based Section Detection**: Uses PyMuPDF to read PDF bookmarks/outlines\n",
        "- **In-Memory/Local Temp Files**: Uses `/tmp` on worker nodes for fast I/O\n",
        "- **Batch AI Processing**: Parallel `ai_parse_document` calls via Spark SQL\n",
        "- **Ordered Content Storage**: Preserves text/table sequence within sections\n",
        "\n",
        "## Performance\n",
        "- Processes multiple sections in parallel across cluster workers\n",
        "- Estimated 5-10x speedup compared to sequential processing\n",
        "\n",
        "## Parameters\n",
        "- `catalog_override`: Unity Catalog name\n",
        "- `sandbox`: If 'true', only extract and store for preview (no downstream processing)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 0: Install required packages for PDF TOC extraction\n",
        "%pip install PyMuPDF pdfminer.six PyPDF2 --quiet\n",
        "# Note: Restart Python kernel if packages were just installed\n",
        "# dbutils.library.restartPython()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Setup and Parameters\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Iterator\n",
        "\n",
        "# PySpark\n",
        "import pandas as pd\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col, lit, expr, current_timestamp\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
        "\n",
        "# Widget parameters\n",
        "dbutils.widgets.text(\"catalog_override\", \"aira_test\")\n",
        "dbutils.widgets.text(\"sandbox\", \"true\")\n",
        "dbutils.widgets.text(\"study_id\", \"\")\n",
        "dbutils.widgets.text(\"protocol_id\", \"\")\n",
        "\n",
        "catalog = dbutils.widgets.get(\"catalog_override\")\n",
        "sandbox_mode = dbutils.widgets.get(\"sandbox\").lower() == \"true\"\n",
        "study_id = dbutils.widgets.get(\"study_id\") or None\n",
        "protocol_id = dbutils.widgets.get(\"protocol_id\") or None\n",
        "\n",
        "print(f\"Study ID: {study_id}\")\n",
        "print(f\"Protocol ID: {protocol_id}\")\n",
        "\n",
        "# Schema names\n",
        "bronze_schema = \"bronze_md\"\n",
        "silver_schema = \"silver_md\"\n",
        "gold_schema = \"gold_md\"\n",
        "\n",
        "# Get job tracking values from upstream setup task\n",
        "try:\n",
        "    databricks_run_id = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"databricks_run_id\")\n",
        "    databricks_job_id = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"databricks_job_id\")\n",
        "    databricks_job_name = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"databricks_job_name\")\n",
        "    created_by_principal = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"created_by_principal\")\n",
        "except Exception as e:\n",
        "    # Fallback for standalone testing\n",
        "    databricks_run_id = \"test_run_\" + datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    databricks_job_id = \"test_job\"\n",
        "    databricks_job_name = \"nb_protocol_processor\"\n",
        "    created_by_principal = \"test_user\"\n",
        "\n",
        "print(f\"Catalog: {catalog}\")\n",
        "print(f\"Sandbox Mode: {sandbox_mode}\")\n",
        "print(f\"Run ID: {databricks_run_id}\")\n",
        "print(f\"Job ID: {databricks_job_id}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Define protocol document sections table schema with ordered content support\n",
        "# Table: silver_md.md_protocol_document_sections\n",
        "# Added study_id and protocol_id for linking to study management\n",
        "\n",
        "SECTIONS_TABLE = f\"{catalog}.{silver_schema}.md_protocol_document_sections\"\n",
        "FILE_HISTORY_TABLE = f\"{catalog}.{bronze_schema}.md_file_history\"\n",
        "\n",
        "# Schema for md_protocol_document_sections - supports ordered content within sections\n",
        "# Each row represents one content item (text or table) within a section\n",
        "# All fields are nullable (True) to prevent NullPointerException during DataFrame creation\n",
        "sections_schema = StructType([\n",
        "    StructField(\"section_id\", StringType(), True),          # Unique ID for the section\n",
        "    StructField(\"document_id\", StringType(), True),         # FK to md_file_history\n",
        "    StructField(\"study_id\", StringType(), True),            # FK to md_study\n",
        "    StructField(\"protocol_id\", StringType(), True),         # FK to md_protocol\n",
        "    StructField(\"section_name\", StringType(), True),        # From TOC (e.g., \"Schedule of Activities\")\n",
        "    StructField(\"section_level\", IntegerType(), True),      # TOC hierarchy level\n",
        "    StructField(\"page_start\", IntegerType(), True),         # Section start page (0-based)\n",
        "    StructField(\"page_end\", IntegerType(), True),           # Section end page (0-based)\n",
        "    StructField(\"content_order\", IntegerType(), True),      # Sequence within section (1, 2, 3...)\n",
        "    StructField(\"content_type\", StringType(), True),        # \"table\" or \"text\"\n",
        "    StructField(\"content_data\", StringType(), True),        # JSON (table: headers/rows, text: content)\n",
        "    StructField(\"created_ts\", TimestampType(), True),       # When extraction was performed\n",
        "    StructField(\"created_by_principal\", StringType(), True),\n",
        "    StructField(\"databricks_run_id\", StringType(), True),\n",
        "    StructField(\"databricks_job_id\", StringType(), True),\n",
        "    StructField(\"databricks_job_name\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Drop and recreate table with new schema (for development)\n",
        "# In production, use ALTER TABLE to add columns\n",
        "try:\n",
        "    # Check if table exists and has old schema\n",
        "    existing_cols = [c.name for c in spark.table(SECTIONS_TABLE).schema]\n",
        "    if 'section_id' not in existing_cols:\n",
        "        print(f\"‚ö†Ô∏è Dropping old table to recreate with new schema...\")\n",
        "        spark.sql(f\"DROP TABLE IF EXISTS {SECTIONS_TABLE}\")\n",
        "except Exception:\n",
        "    pass  # Table doesn't exist, will be created below\n",
        "\n",
        "# Create protocol documents details table with new schema\n",
        "try:\n",
        "    spark.sql(f\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS {SECTIONS_TABLE} (\n",
        "            section_id STRING COMMENT 'Unique ID (UUID) for the section',\n",
        "            document_id STRING COMMENT 'FK to md_file_history.document_id',\n",
        "            study_id STRING COMMENT 'FK to md_study.study_id',\n",
        "            protocol_id STRING COMMENT 'FK to md_protocol.protocol_id',\n",
        "            section_name STRING COMMENT 'Section name from TOC (e.g., Schedule of Activities)',\n",
        "            section_level INT COMMENT 'TOC hierarchy level (1=top level)',\n",
        "            page_start INT COMMENT 'Section start page (0-based)',\n",
        "            page_end INT COMMENT 'Section end page (0-based)',\n",
        "            content_order INT COMMENT 'Sequence within section (1, 2, 3...) to preserve order',\n",
        "            content_type STRING COMMENT 'Content type: table or text',\n",
        "            content_data STRING COMMENT 'JSON content - for table: headers/rows, for text: content string',\n",
        "            created_ts TIMESTAMP COMMENT 'When extraction was performed',\n",
        "            created_by_principal STRING COMMENT 'User or service principal',\n",
        "            databricks_run_id STRING COMMENT 'Job run ID for lineage',\n",
        "            databricks_job_id STRING COMMENT 'Job ID for lineage',\n",
        "            databricks_job_name STRING COMMENT 'Job name for lineage'\n",
        "        )\n",
        "        USING DELTA\n",
        "        COMMENT 'Protocol document details storing ordered content from extracted sections'\n",
        "        TBLPROPERTIES (\n",
        "            'delta.enableChangeDataFeed' = 'false'\n",
        "        )\n",
        "    \"\"\")\n",
        "    print(f\"‚úÖ Table {SECTIONS_TABLE} ready\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Table creation: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: TOC Extraction - Using Framework Module\n",
        "# Import the PDFTOCExtractor from the clinical_data_standards_framework\n",
        "# This provides a clean abstraction for PDF TOC extraction with section parsing\n",
        "\n",
        "import fitz  # PyMuPDF - needed for page count fallback when no TOC found\n",
        "\n",
        "from clinical_data_standards_framework.toc_utils import PDFTOCExtractor, Section\n",
        "\n",
        "# Initialize the PDF TOC extractor\n",
        "pdf_toc_extractor = PDFTOCExtractor()\n",
        "\n",
        "print(\"‚úÖ TOC extraction loaded from clinical_data_standards_framework.toc_utils\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Get pending Protocol documents\n",
        "# (Legacy functions removed - now using framework module and inline logic in parallel phases)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Get pending Protocol documents and build sections DataFrame\n",
        "\n",
        "# ============================================================================\n",
        "# Load section filter configuration from task values (set by setup task)\n",
        "# ============================================================================\n",
        "try:\n",
        "    pipeline_config_json = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"pipeline_config\")\n",
        "    pipeline_config = json.loads(pipeline_config_json) if pipeline_config_json else {}\n",
        "    \n",
        "    ai_processing_config = pipeline_config.get('ai_processing', {})\n",
        "    section_filter_config = ai_processing_config.get('section_filter', {})\n",
        "    target_sections = ai_processing_config.get('target_sections', [])\n",
        "    \n",
        "    # Section filter settings\n",
        "    filter_enabled = section_filter_config.get('enabled', False)\n",
        "    max_level = section_filter_config.get('max_level', None)\n",
        "    process_all_if_no_match = section_filter_config.get('process_all_if_no_match', False)\n",
        "    \n",
        "    # Build regex patterns from target sections\n",
        "    section_patterns = [ts.get('pattern', '') for ts in target_sections if ts.get('pattern')]\n",
        "    \n",
        "    print(f\"üìã Section Filter Config (from task values):\")\n",
        "    print(f\"   Enabled: {filter_enabled}\")\n",
        "    print(f\"   Max Level: {max_level or 'All'}\")\n",
        "    print(f\"   Patterns: {section_patterns}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Could not load config from task values, processing all sections: {e}\")\n",
        "    filter_enabled = False\n",
        "    max_level = None\n",
        "    section_patterns = []\n",
        "    process_all_if_no_match = True\n",
        "\n",
        "def section_matches_filter(section_title: str, patterns: List[str]) -> bool:\n",
        "    \"\"\"Check if section title matches any of the filter patterns.\"\"\"\n",
        "    if not patterns:\n",
        "        return True\n",
        "    for pattern in patterns:\n",
        "        if re.search(pattern, section_title, re.IGNORECASE):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "# Query for Protocol documents that are ready for processing\n",
        "pending_docs_query = f\"\"\"\n",
        "    SELECT \n",
        "        document_id,\n",
        "        extracted_path,\n",
        "        SPLIT(extracted_path, '/')[SIZE(SPLIT(extracted_path, '/')) - 1] as file_name,\n",
        "        file_extension,\n",
        "        document_tags,\n",
        "        status,\n",
        "        databricks_run_id as source_run_id\n",
        "    FROM {FILE_HISTORY_TABLE}\n",
        "    WHERE array_contains(document_tags, 'Protocol')\n",
        "      AND status = 'READY_FOR_PROCESSING'\n",
        "      AND file_extension IN ('.pdf', '.docx', '.doc')\n",
        "    ORDER BY created_ts DESC\n",
        "\"\"\"\n",
        "\n",
        "pending_docs_df = spark.sql(pending_docs_query)\n",
        "pending_count = pending_docs_df.count()\n",
        "\n",
        "print(f\"\\nFound {pending_count} Protocol documents to process\")\n",
        "if pending_count > 0:\n",
        "    pending_docs_df.show(truncate=False)\n",
        "\n",
        "# Exit early if no documents\n",
        "if pending_count == 0:\n",
        "    print(\"No Protocol documents to process. Exiting.\")\n",
        "    dbutils.notebook.exit(json.dumps({\"status\": \"success\", \"documents_processed\": 0}))\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: Extract TOC sections from all documents (on driver)\n",
        "# This creates a DataFrame with one row per section\n",
        "# Applies section filtering based on config (patterns and max_level)\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 1: Extracting TOC sections from all documents...\")\n",
        "if filter_enabled:\n",
        "    print(f\"   (Filtering to: {section_patterns})\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "sections_data = []\n",
        "processed_doc_ids = []\n",
        "failed_doc_ids = []\n",
        "total_sections_found = 0\n",
        "sections_after_filter = 0\n",
        "\n",
        "for doc in pending_docs_df.collect():\n",
        "    document_id = doc['document_id']\n",
        "    extracted_path = doc['extracted_path']\n",
        "    file_name = doc['file_name']\n",
        "    \n",
        "    print(f\"\\nüìÑ {file_name}\")\n",
        "    \n",
        "    try:\n",
        "        # Extract all sections from TOC using the framework extractor\n",
        "        sections = pdf_toc_extractor.extract_sections(extracted_path)\n",
        "        \n",
        "        if not sections:\n",
        "            print(f\"  ‚ö†Ô∏è No TOC found, treating as single section\")\n",
        "            # Get total pages for full document\n",
        "            with fitz.open(extracted_path) as doc_pdf:\n",
        "                total_pages = doc_pdf.page_count\n",
        "            sections = [Section(\n",
        "                title=\"Full Document\",\n",
        "                level=1,\n",
        "                page_start=0,\n",
        "                page_end=total_pages - 1\n",
        "            )]\n",
        "        \n",
        "        total_sections_found += len(sections)\n",
        "        print(f\"  üìã Found {len(sections)} sections in TOC\")\n",
        "        \n",
        "        # Apply section filtering\n",
        "        matched_sections = []\n",
        "        for section in sections:\n",
        "            # Apply max_level filter if configured\n",
        "            if max_level and section.level > max_level:\n",
        "                continue\n",
        "            \n",
        "            # Apply pattern filter if enabled\n",
        "            if filter_enabled and section_patterns:\n",
        "                if section_matches_filter(section.title, section_patterns):\n",
        "                    matched_sections.append(section)\n",
        "            else:\n",
        "                matched_sections.append(section)\n",
        "        \n",
        "        if filter_enabled:\n",
        "            print(f\"  ‚úÖ After filter: {len(matched_sections)} sections match patterns\")\n",
        "        \n",
        "        # If no matches and process_all_if_no_match is False, skip this document\n",
        "        if filter_enabled and not matched_sections and not process_all_if_no_match:\n",
        "            print(f\"  ‚ö†Ô∏è No sections match filter patterns, skipping document\")\n",
        "            continue\n",
        "        \n",
        "        # Use matched sections (or all if filter not enabled/no matches)\n",
        "        sections_to_process = matched_sections if matched_sections else (sections if process_all_if_no_match else [])\n",
        "        \n",
        "        for section in sections_to_process:\n",
        "            section_id = str(uuid.uuid4())\n",
        "            sections_data.append({\n",
        "                'section_id': section_id,\n",
        "                'document_id': document_id,\n",
        "                'source_path': extracted_path,\n",
        "                'section_title': section.title,\n",
        "                'section_level': int(section.level),\n",
        "                'page_start': int(section.page_start),\n",
        "                'page_end': int(section.page_end) if section.page_end else 999\n",
        "            })\n",
        "        \n",
        "        sections_after_filter += len(sections_to_process)\n",
        "        processed_doc_ids.append(document_id)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error extracting TOC: {e}\")\n",
        "        failed_doc_ids.append(document_id)\n",
        "\n",
        "print(f\"\\n‚úÖ Sections: {total_sections_found} found ‚Üí {sections_after_filter} after filter\")\n",
        "print(f\"‚úÖ Created {len(sections_data)} section records from {len(processed_doc_ids)} documents\")\n",
        "\n",
        "# Create sections DataFrame\n",
        "sections_schema = StructType([\n",
        "    StructField(\"section_id\", StringType(), False),\n",
        "    StructField(\"document_id\", StringType(), False),\n",
        "    StructField(\"source_path\", StringType(), False),\n",
        "    StructField(\"section_title\", StringType(), True),\n",
        "    StructField(\"section_level\", IntegerType(), True),\n",
        "    StructField(\"page_start\", IntegerType(), True),\n",
        "    StructField(\"page_end\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "sections_df = spark.createDataFrame(sections_data, schema=sections_schema)\n",
        "\n",
        "# Repartition for parallel processing (aim for ~4-8 sections per partition)\n",
        "num_partitions = max(1, len(sections_data) // 6)\n",
        "num_partitions = min(num_partitions, 16)  # Cap at 16 partitions\n",
        "sections_df = sections_df.repartition(num_partitions)\n",
        "\n",
        "print(f\"üìä Sections DataFrame: {sections_df.count()} rows, {num_partitions} partitions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: PHASE 1 - Extract PDF sections to temp Volume files (parallel, no Spark SQL)\n",
        "\n",
        "# ============================================================================\n",
        "# TWO-PHASE APPROACH for Serverless Compute:\n",
        "# Phase 1: mapInPandas extracts PDF sections to temp files (parallel, no SQL)\n",
        "# Phase 2: Spark SQL calls ai_parse_document on all files (parallel via SQL)\n",
        "# ============================================================================\n",
        "\n",
        "# Get temp volume path from first document\n",
        "first_doc = pending_docs_df.first()\n",
        "base_volume_path = '/'.join(first_doc['extracted_path'].split('/')[:-2])\n",
        "TEMP_SECTIONS_PATH = f\"{base_volume_path}/temp_sections/{databricks_run_id}\"\n",
        "\n",
        "# Create temp directory\n",
        "os.makedirs(TEMP_SECTIONS_PATH, exist_ok=True)\n",
        "print(f\"üìÅ Temp sections path: {TEMP_SECTIONS_PATH}\")\n",
        "\n",
        "# Schema for Phase 1 output (extracted files)\n",
        "extraction_schema = StructType([\n",
        "    StructField(\"section_id\", StringType(), False),\n",
        "    StructField(\"document_id\", StringType(), False),\n",
        "    StructField(\"section_title\", StringType(), True),\n",
        "    StructField(\"section_level\", IntegerType(), True),\n",
        "    StructField(\"page_start\", IntegerType(), True),\n",
        "    StructField(\"page_end\", IntegerType(), True),\n",
        "    StructField(\"temp_file_path\", StringType(), True),\n",
        "    StructField(\"extraction_status\", StringType(), True),\n",
        "    StructField(\"error_message\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Broadcast the temp path to workers\n",
        "temp_path_broadcast = TEMP_SECTIONS_PATH\n",
        "\n",
        "def extract_sections_to_files(pdf_iterator: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    PHASE 1: Extract PDF sections to temp Volume files.\n",
        "    NO Spark SQL - just pure Python/Pandas file operations.\n",
        "    Runs in parallel across worker nodes.\n",
        "    \"\"\"\n",
        "    import os\n",
        "    from PyPDF2 import PdfReader, PdfWriter\n",
        "    \n",
        "    for pdf in pdf_iterator:\n",
        "        results = []\n",
        "        \n",
        "        for _, row in pdf.iterrows():\n",
        "            section_id = row['section_id']\n",
        "            document_id = row['document_id']\n",
        "            source_path = row['source_path']\n",
        "            section_title = row['section_title']\n",
        "            section_level = row['section_level']\n",
        "            page_start = row['page_start']\n",
        "            page_end = row['page_end']\n",
        "            \n",
        "            # Use Volume path for temp files (accessible by Spark SQL in Phase 2)\n",
        "            temp_file_path = f\"{temp_path_broadcast}/{section_id}.pdf\"\n",
        "            \n",
        "            try:\n",
        "                # Extract section pages to temp file\n",
        "                with open(source_path, 'rb') as infile:\n",
        "                    reader = PdfReader(infile)\n",
        "                    writer = PdfWriter()\n",
        "                    \n",
        "                    total_pages = len(reader.pages)\n",
        "                    actual_end = min(page_end, total_pages - 1)\n",
        "                    \n",
        "                    for page_num in range(page_start, actual_end + 1):\n",
        "                        writer.add_page(reader.pages[page_num])\n",
        "                    \n",
        "                    # Ensure directory exists\n",
        "                    os.makedirs(os.path.dirname(temp_file_path), exist_ok=True)\n",
        "                    \n",
        "                    with open(temp_file_path, 'wb') as outfile:\n",
        "                        writer.write(outfile)\n",
        "                \n",
        "                results.append({\n",
        "                    'section_id': section_id,\n",
        "                    'document_id': document_id,\n",
        "                    'section_title': section_title,\n",
        "                    'section_level': section_level,\n",
        "                    'page_start': page_start,\n",
        "                    'page_end': page_end,\n",
        "                    'temp_file_path': temp_file_path,\n",
        "                    'extraction_status': 'EXTRACTED',\n",
        "                    'error_message': None\n",
        "                })\n",
        "                \n",
        "            except Exception as e:\n",
        "                results.append({\n",
        "                    'section_id': section_id,\n",
        "                    'document_id': document_id,\n",
        "                    'section_title': section_title,\n",
        "                    'section_level': section_level,\n",
        "                    'page_start': page_start,\n",
        "                    'page_end': page_end,\n",
        "                    'temp_file_path': None,\n",
        "                    'extraction_status': 'FAILED',\n",
        "                    'error_message': str(e)[:500]\n",
        "                })\n",
        "        \n",
        "        yield pd.DataFrame(results) if results else pd.DataFrame(columns=[\n",
        "            'section_id', 'document_id', 'section_title', 'section_level',\n",
        "            'page_start', 'page_end', 'temp_file_path', 'extraction_status', 'error_message'\n",
        "        ])\n",
        "\n",
        "print(\"‚úÖ Phase 1 extraction function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Execute PHASE 1 - Extract sections to temp files (parallel)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PHASE 1: Extracting PDF sections in parallel...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "import time\n",
        "phase1_start = time.time()\n",
        "\n",
        "# Apply Phase 1 extraction function via mapInPandas\n",
        "extracted_df = sections_df.mapInPandas(\n",
        "    extract_sections_to_files,\n",
        "    schema=extraction_schema\n",
        ")\n",
        "\n",
        "# Write to temp view to materialize the extraction\n",
        "extracted_df.createOrReplaceTempView(\"extracted_sections_temp\")\n",
        "\n",
        "# Force materialization and get counts\n",
        "extraction_results = spark.sql(\"\"\"\n",
        "    SELECT \n",
        "        extraction_status,\n",
        "        COUNT(*) as count\n",
        "    FROM extracted_sections_temp\n",
        "    GROUP BY extraction_status\n",
        "\"\"\").collect()\n",
        "\n",
        "phase1_elapsed = time.time() - phase1_start\n",
        "\n",
        "print(f\"\\n‚úÖ Phase 1 complete in {phase1_elapsed:.1f} seconds\")\n",
        "for row in extraction_results:\n",
        "    print(f\"  {row['extraction_status']}: {row['count']} sections\")\n",
        "\n",
        "# Get successfully extracted sections\n",
        "extracted_count = spark.sql(\"SELECT COUNT(*) FROM extracted_sections_temp WHERE extraction_status = 'EXTRACTED'\").collect()[0][0]\n",
        "print(f\"\\nüìä {extracted_count} sections ready for AI parsing\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8: Execute PHASE 2 - Call ai_parse_document via DataFrame API (parallel)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PHASE 2: Parsing sections with ai_parse_document...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "phase2_start = time.time()\n",
        "\n",
        "# Get the list of extracted file paths\n",
        "extracted_paths_df = spark.sql(\"\"\"\n",
        "    SELECT section_id, document_id, section_title, section_level, \n",
        "           page_start, page_end, temp_file_path\n",
        "    FROM extracted_sections_temp\n",
        "    WHERE extraction_status = 'EXTRACTED'\n",
        "\"\"\")\n",
        "\n",
        "extracted_paths = extracted_paths_df.collect()\n",
        "print(f\"üìÑ Processing {len(extracted_paths)} extracted sections...\")\n",
        "\n",
        "if extracted_paths:\n",
        "    # Create metadata DataFrame for joining\n",
        "    paths_df = spark.createDataFrame(\n",
        "        [(row['section_id'], row['document_id'], row['section_title'], \n",
        "          row['section_level'], row['page_start'], row['page_end'], \n",
        "          row['temp_file_path']) for row in extracted_paths],\n",
        "        ['section_id', 'document_id', 'section_title', 'section_level', \n",
        "         'page_start', 'page_end', 'temp_file_path']\n",
        "    )\n",
        "    \n",
        "    # Read all binary files using DataFrame API (Spark parallelizes this)\n",
        "    file_paths = [row['temp_file_path'] for row in extracted_paths]\n",
        "    binary_df = spark.read.format(\"binaryFile\").load(file_paths)\n",
        "    \n",
        "    # Join binary content with section metadata using filename (equi-join)\n",
        "    # Extract filename from path for matching (handles different path prefixes like file:/, dbfs:/)\n",
        "    binary_df = binary_df.withColumn(\n",
        "        \"filename\", \n",
        "        F.element_at(F.split(\"path\", \"/\"), -1)\n",
        "    )\n",
        "    \n",
        "    paths_df = paths_df.withColumn(\n",
        "        \"filename\",\n",
        "        F.element_at(F.split(\"temp_file_path\", \"/\"), -1)\n",
        "    )\n",
        "    \n",
        "    # Equi-join on filename (fast hash join)\n",
        "    binary_with_meta = binary_df.join(\n",
        "        paths_df,\n",
        "        binary_df.filename == paths_df.filename,\n",
        "        \"inner\"\n",
        "    )\n",
        "    \n",
        "    # Apply ai_parse_document using expr (runs in parallel across cluster)\n",
        "    parsed_df = binary_with_meta.withColumn(\n",
        "        \"parsed_result\",\n",
        "        expr(\"ai_parse_document(content, map('version', '2.0', 'descriptionElementTypes', '*'))\")\n",
        "    )\n",
        "    \n",
        "    # Extract elements as string for processing\n",
        "    parsed_elements_df = parsed_df.selectExpr(\n",
        "        \"section_id\",\n",
        "        \"document_id\",\n",
        "        \"section_title\",\n",
        "        \"section_level\",\n",
        "        \"page_start\",\n",
        "        \"page_end\",\n",
        "        \"temp_file_path\",\n",
        "        \"CAST(parsed_result:document:elements AS STRING) as elements_json\",\n",
        "        \"CAST(parsed_result:error_status AS STRING) as error_status\"\n",
        "    )\n",
        "    \n",
        "    # Collect results NOW to avoid re-computation in Phase 3\n",
        "    # This is the single point where ai_parse_document is executed\n",
        "    parsed_rows = parsed_elements_df.collect()\n",
        "    \n",
        "    phase2_elapsed = time.time() - phase2_start\n",
        "    parsed_count = len(parsed_rows)\n",
        "    \n",
        "    print(f\"\\n‚úÖ Phase 2 complete in {phase2_elapsed:.1f} seconds\")\n",
        "    print(f\"üìä Parsed {parsed_count} sections\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No sections to parse\")\n",
        "    phase2_elapsed = 0\n",
        "    parsed_rows = []  # Empty list for Phase 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 9: PHASE 3 - Process parsed results and write to sandbox table\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PHASE 3: Processing parsed content and writing to sandbox...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "phase3_start = time.time()\n",
        "\n",
        "# Use parsed_rows collected in Phase 2 (avoids re-computation!)\n",
        "# Filter out errors\n",
        "parsed_rows_success = [row for row in parsed_rows \n",
        "                       if row['error_status'] is None or row['error_status'] == 'null']\n",
        "\n",
        "print(f\"üìÑ Processing {len(parsed_rows_success)} successfully parsed sections...\")\n",
        "\n",
        "# Process elements and create content rows\n",
        "all_content_rows_data = []\n",
        "\n",
        "for row in parsed_rows_success:\n",
        "    section_id = row['section_id']\n",
        "    document_id = row['document_id']\n",
        "    section_title = row['section_title']\n",
        "    section_level = row['section_level']\n",
        "    page_start = row['page_start']\n",
        "    page_end = row['page_end']\n",
        "    elements_json = row['elements_json']\n",
        "    \n",
        "    if not elements_json:\n",
        "        continue\n",
        "    \n",
        "    try:\n",
        "        elements = json.loads(elements_json)\n",
        "        \n",
        "        # Process elements in order\n",
        "        ordered_content = []\n",
        "        for elem in elements:\n",
        "            elem_type = elem.get('type', '').lower()\n",
        "            content = elem.get('content', '')\n",
        "            \n",
        "            if elem_type == 'table':\n",
        "                if isinstance(content, str) and '<table' in content.lower():\n",
        "                    # Parse HTML table\n",
        "                    th_matches = re.findall(r'<th[^>]*>(.*?)</th>', content, re.IGNORECASE | re.DOTALL)\n",
        "                    headers = [re.sub(r'<[^>]+>', '', h).strip() for h in th_matches]\n",
        "                    \n",
        "                    tr_matches = re.findall(r'<tr[^>]*>(.*?)</tr>', content, re.IGNORECASE | re.DOTALL)\n",
        "                    rows = []\n",
        "                    for tr in tr_matches:\n",
        "                        if '<th' in tr.lower() and headers:\n",
        "                            continue\n",
        "                        td_matches = re.findall(r'<td[^>]*>(.*?)</td>', tr, re.IGNORECASE | re.DOTALL)\n",
        "                        if td_matches:\n",
        "                            row_data = [re.sub(r'<[^>]+>', '', td).strip() for td in td_matches]\n",
        "                            rows.append(row_data)\n",
        "                    \n",
        "                    ordered_content.append({\n",
        "                        'type': 'table',\n",
        "                        'data': {\n",
        "                            'html': content,\n",
        "                            'headers': headers,\n",
        "                            'rows': rows,\n",
        "                            'row_count': len(rows),\n",
        "                            'column_count': len(headers)\n",
        "                        }\n",
        "                    })\n",
        "            elif elem_type in ('paragraph', 'text', 'title', 'section_header', 'line', 'caption'):\n",
        "                text_val = elem.get('text', '') or content\n",
        "                if isinstance(text_val, str) and text_val.strip():\n",
        "                    # Merge consecutive text elements\n",
        "                    if ordered_content and ordered_content[-1]['type'] == 'text':\n",
        "                        ordered_content[-1]['data']['content'] += '\\n' + text_val\n",
        "                    else:\n",
        "                        ordered_content.append({\n",
        "                            'type': 'text',\n",
        "                            'data': {'content': text_val}\n",
        "                        })\n",
        "        \n",
        "        # Create content rows (including study_id and protocol_id from job parameters)\n",
        "        for content_order, content_item in enumerate(ordered_content, start=1):\n",
        "            all_content_rows_data.append({\n",
        "                \"section_id\": section_id,\n",
        "                \"document_id\": document_id,\n",
        "                \"study_id\": study_id,\n",
        "                \"protocol_id\": protocol_id,\n",
        "                \"section_name\": section_title,\n",
        "                \"section_level\": section_level,\n",
        "                \"page_start\": page_start,\n",
        "                \"page_end\": page_end,\n",
        "                \"content_order\": content_order,\n",
        "                \"content_type\": content_item['type'],\n",
        "                \"content_data\": json.dumps(content_item['data']),\n",
        "                \"created_ts\": datetime.now(),\n",
        "                \"created_by_principal\": created_by_principal,\n",
        "                \"databricks_run_id\": databricks_run_id,\n",
        "                \"databricks_job_id\": databricks_job_id,\n",
        "                \"databricks_job_name\": databricks_job_name\n",
        "            })\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö†Ô∏è Error processing section {section_id}: {e}\")\n",
        "\n",
        "print(f\"‚úÖ Created {len(all_content_rows_data)} content rows\")\n",
        "\n",
        "# Write to protocol document sections table\n",
        "if all_content_rows_data:\n",
        "    # Debug: Print first row to verify data types\n",
        "    print(\"\\nüìã DEBUG - First row sample:\")\n",
        "    first_row = all_content_rows_data[0]\n",
        "    for key, value in first_row.items():\n",
        "        val_type = type(value).__name__\n",
        "        val_preview = str(value)[:50] if value else \"None\"\n",
        "        print(f\"  {key}: {val_preview}... ({val_type})\")\n",
        "    \n",
        "    # Check for any NULL in critical fields\n",
        "    null_issues = []\n",
        "    for i, row in enumerate(all_content_rows_data):\n",
        "        if row.get('section_id') is None:\n",
        "            null_issues.append(f\"Row {i}: section_id is None\")\n",
        "        if row.get('document_id') is None:\n",
        "            null_issues.append(f\"Row {i}: document_id is None\")\n",
        "    \n",
        "    if null_issues:\n",
        "        print(f\"\\n‚ö†Ô∏è Found {len(null_issues)} NULL issues:\")\n",
        "        for issue in null_issues[:5]:  # Show first 5\n",
        "            print(f\"  {issue}\")\n",
        "    else:\n",
        "        print(\"\\n‚úÖ No NULL issues in critical fields\")\n",
        "    \n",
        "    # Don't pass explicit schema - let Spark infer it to avoid NULL serialization issues\n",
        "    # Use mergeSchema to handle any schema evolution\n",
        "    content_df = spark.createDataFrame(all_content_rows_data)\n",
        "    \n",
        "    # Explicitly cast integer columns to INT to avoid LONG vs INT schema conflicts\n",
        "    # Spark may infer these as LONG (64-bit), but table schema expects INT (32-bit)\n",
        "    from pyspark.sql.functions import col\n",
        "    content_df = content_df \\\n",
        "        .withColumn(\"section_level\", col(\"section_level\").cast(\"int\")) \\\n",
        "        .withColumn(\"page_start\", col(\"page_start\").cast(\"int\")) \\\n",
        "        .withColumn(\"page_end\", col(\"page_end\").cast(\"int\")) \\\n",
        "        .withColumn(\"content_order\", col(\"content_order\").cast(\"int\"))\n",
        "    \n",
        "    content_df.write \\\n",
        "        .mode(\"append\") \\\n",
        "        .option(\"mergeSchema\", \"true\") \\\n",
        "        .saveAsTable(SECTIONS_TABLE)\n",
        "    \n",
        "    all_content_rows = len(all_content_rows_data)\n",
        "    print(f\"‚úÖ Wrote {all_content_rows} rows to {SECTIONS_TABLE}\")\n",
        "else:\n",
        "    all_content_rows = 0\n",
        "    print(\"‚ö†Ô∏è No content to write\")\n",
        "\n",
        "phase3_elapsed = time.time() - phase3_start\n",
        "print(f\"\\n‚úÖ Phase 3 complete in {phase3_elapsed:.1f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 10: Cleanup temp files\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Cleaning up temp files...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "import shutil\n",
        "import glob\n",
        "\n",
        "try:\n",
        "    # Check if temp directory exists\n",
        "    if os.path.exists(TEMP_SECTIONS_PATH):\n",
        "        # Count files before deletion\n",
        "        temp_files = glob.glob(f\"{TEMP_SECTIONS_PATH}/*.pdf\")\n",
        "        file_count = len(temp_files)\n",
        "        print(f\"üìÅ Found {file_count} temp PDF files to delete\")\n",
        "        \n",
        "        # Remove entire temp directory\n",
        "        shutil.rmtree(TEMP_SECTIONS_PATH)\n",
        "        print(f\"‚úÖ Removed temp directory: {TEMP_SECTIONS_PATH}\")\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è No temp directory to clean up\")\n",
        "        \n",
        "    # Also check parent temp_sections folder and clean up old runs if empty\n",
        "    parent_temp_path = os.path.dirname(TEMP_SECTIONS_PATH)\n",
        "    if os.path.exists(parent_temp_path):\n",
        "        remaining_dirs = os.listdir(parent_temp_path)\n",
        "        if not remaining_dirs:\n",
        "            os.rmdir(parent_temp_path)\n",
        "            print(f\"‚úÖ Removed empty parent directory: {parent_temp_path}\")\n",
        "        else:\n",
        "            print(f\"‚ÑπÔ∏è Parent directory has {len(remaining_dirs)} other run directories\")\n",
        "            \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error cleaning up temp files: {e}\")\n",
        "\n",
        "# Print total processing time\n",
        "total_elapsed = phase1_elapsed + phase2_elapsed + phase3_elapsed\n",
        "print(f\"\\nüìä Total processing time: {total_elapsed:.1f} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 11: Update document status in md_file_history\n",
        "\n",
        "# processed_doc_ids and failed_doc_ids are defined in Cell 6 (STEP 1)\n",
        "\n",
        "if processed_doc_ids:\n",
        "    ids_str = \"', '\".join(processed_doc_ids)\n",
        "    update_query = f\"\"\"\n",
        "        UPDATE {FILE_HISTORY_TABLE}\n",
        "        SET \n",
        "            status = 'PROTOCOL_COMPLETED',\n",
        "            status_timestamp = current_timestamp(),\n",
        "            last_updated_ts = current_timestamp(),\n",
        "            last_updated_by_principal = '{created_by_principal}',\n",
        "            databricks_job_id = '{databricks_job_id}',\n",
        "            databricks_job_name = '{databricks_job_name}',\n",
        "            databricks_run_id = '{databricks_run_id}'\n",
        "        WHERE document_id IN ('{ids_str}')\n",
        "    \"\"\"\n",
        "    spark.sql(update_query)\n",
        "    print(f\"‚úÖ Updated {len(processed_doc_ids)} documents to PROTOCOL_COMPLETED\")\n",
        "\n",
        "if failed_doc_ids:\n",
        "    ids_str = \"', '\".join(failed_doc_ids)\n",
        "    update_query = f\"\"\"\n",
        "        UPDATE {FILE_HISTORY_TABLE}\n",
        "        SET \n",
        "            status = 'PROTOCOL_FAILED',\n",
        "            status_timestamp = current_timestamp(),\n",
        "            last_updated_ts = current_timestamp(),\n",
        "            last_updated_by_principal = '{created_by_principal}',\n",
        "            databricks_job_id = '{databricks_job_id}',\n",
        "            databricks_job_name = '{databricks_job_name}',\n",
        "            databricks_run_id = '{databricks_run_id}'\n",
        "        WHERE document_id IN ('{ids_str}')\n",
        "    \"\"\"\n",
        "    spark.sql(update_query)\n",
        "    print(f\"‚ö†Ô∏è Updated {len(failed_doc_ids)} documents to PROTOCOL_FAILED\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 12: Update protocol draft status and exit with summary\n",
        "\n",
        "# Update md_protocol_draft status if protocol_id is provided\n",
        "if protocol_id:\n",
        "    protocol_draft_table = f\"{catalog}.{silver_schema}.md_protocol_draft\"\n",
        "    try:\n",
        "        if len(failed_doc_ids) > 0 and len(processed_doc_ids) == 0:\n",
        "            new_status = \"FAILED\"\n",
        "        elif all_content_rows > 0:\n",
        "            new_status = \"SECTIONS_EXTRACTED\"\n",
        "        else:\n",
        "            new_status = \"PARSING\"\n",
        "        \n",
        "        spark.sql(f\"\"\"\n",
        "            UPDATE {protocol_draft_table}\n",
        "            SET status = '{new_status}',\n",
        "                section_count = {all_content_rows},\n",
        "                last_updated_by_principal = '{created_by_principal}',\n",
        "                last_updated_ts = current_timestamp()\n",
        "            WHERE protocol_id = '{protocol_id}'\n",
        "        \"\"\")\n",
        "        print(f\"‚úÖ Updated protocol {protocol_id} status to {new_status}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not update protocol draft status: {e}\")\n",
        "\n",
        "# Determine overall status based on results\n",
        "if len(failed_doc_ids) > 0 and len(processed_doc_ids) == 0:\n",
        "    overall_status = \"failed\"\n",
        "elif len(failed_doc_ids) > 0:\n",
        "    overall_status = \"partial_success\"\n",
        "else:\n",
        "    overall_status = \"success\"\n",
        "\n",
        "summary = {\n",
        "    \"status\": overall_status,\n",
        "    \"sandbox_mode\": sandbox_mode,\n",
        "    \"documents_processed\": len(processed_doc_ids),\n",
        "    \"documents_failed\": len(failed_doc_ids),\n",
        "    \"content_rows_extracted\": all_content_rows,\n",
        "    \"sections_table\": SECTIONS_TABLE,\n",
        "    \"run_id\": databricks_run_id,\n",
        "    \"processing_mode\": \"two_phase_parallel\",\n",
        "    \"total_time_seconds\": round(total_elapsed, 1)\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Protocol Processor Summary (Two-Phase Parallel)\")\n",
        "print(\"=\"*60)\n",
        "for key, value in summary.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Fail the job if all documents failed\n",
        "if overall_status == \"failed\":\n",
        "    raise Exception(f\"Protocol processing failed for all {len(failed_doc_ids)} documents.\")\n",
        "\n",
        "dbutils.notebook.exit(json.dumps(summary))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
