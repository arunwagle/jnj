{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# OA Parse Raw\n",
        "Parse Operational Agreement documents from `md_file_history` using `ai_parse_document()` and write to Bronze table.\n",
        "\n",
        "**Input**: `bronze_md.md_file_history` (documents tagged with OPERATIONAL_AGREEMENT)\n",
        "\n",
        "**Output**: `bronze_md.md_oa_file_raw`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 0: Imports\n",
        "import json\n",
        "from pyspark.sql import functions as F\n",
        "from clinical_data_standards_framework.utils import save_with_audit\n",
        "\n",
        "print(\"✅ Framework loaded successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 1: Read config & globals from setup task\n",
        "\n",
        "globals_dict = json.loads(\n",
        "    dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"globals\")\n",
        ")\n",
        "services_dict = json.loads(\n",
        "    dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"services\")\n",
        ")\n",
        "\n",
        "# Debug print: show we really got something\n",
        "print(\"Globals from YAML:\", globals_dict)\n",
        "print(\"Services from YAML:\", list(services_dict.keys()))\n",
        "\n",
        "flow_name = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"flow_name\")\n",
        "print(f\"Flow from setup: {flow_name}\")\n",
        "\n",
        "# Audit globals\n",
        "created_by_principal = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"created_by_principal\")\n",
        "databricks_job_id    = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"databricks_job_id\")\n",
        "databricks_job_name  = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"databricks_job_name\")\n",
        "databricks_run_id    = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"databricks_run_id\")\n",
        "\n",
        "# Try to get pipeline_config; fail with clear message if missing\n",
        "try:\n",
        "    pipeline_config_raw = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"pipeline_config\")\n",
        "    print(f\"pipeline_config (raw from taskValues): {pipeline_config_raw[:200]}...\")\n",
        "    if not pipeline_config_raw:\n",
        "        raise ValueError(\"pipeline_config in taskValues is empty\")\n",
        "    pipeline_config = json.loads(pipeline_config_raw)\n",
        "except Exception as e:\n",
        "    raise ValueError(\n",
        "        \"pipeline_config task value is missing or invalid. \"\n",
        "        \"Make sure:\\n\"\n",
        "        \"  1) The 'setup' task ran successfully, and\\n\"\n",
        "        \"  2) flow_name='operational_agreement_processor' exists in your YAML under 'pipelines', and\\n\"\n",
        "        \"  3) job_populate_config_cache has been run after YAML changes.\\n\"\n",
        "        f\"Root cause: {e}\"\n",
        "    )\n",
        "\n",
        "print(f\"Top-level keys in pipeline_config: {list(pipeline_config.keys())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Resolve OA pipeline configuration\n",
        "\n",
        "docs_cfg   = pipeline_config[\"documents\"][\"operational_agreement\"]\n",
        "source_cfg = docs_cfg[\"source\"]\n",
        "output_cfg = docs_cfg[\"output\"]\n",
        "\n",
        "# From globals\n",
        "catalog       = globals_dict[\"catalog\"]        # e.g. \"dta_poc\"\n",
        "bronze_schema = globals_dict[\"bronze_schema\"]  # e.g. \"bronze_md\"\n",
        "\n",
        "# From pipeline-specific config\n",
        "source_table_name = source_cfg[\"source_table\"]          # e.g. \"md_dta_history\"\n",
        "filter_tags       = source_cfg.get(\"filter_tags\", [])   # list of tags\n",
        "filter_active     = source_cfg.get(\"filter_active\", True)\n",
        "filter_status     = source_cfg.get(\"filter_status\")     # e.g. \"READY_FOR_PROCESSING\"\n",
        "file_extensions   = source_cfg.get(\"file_extensions\", [\".docx\"])\n",
        "\n",
        "raw_table_name = output_cfg[\"raw_table_name\"]           # e.g. \"md_oa_file_raw\"\n",
        "\n",
        "source_table_full = f\"{catalog}.{bronze_schema}.{source_table_name}\"\n",
        "raw_table_full    = f\"{catalog}.{bronze_schema}.{raw_table_name}\"\n",
        "\n",
        "print(\"\\nResolved OA config:\")\n",
        "print(f\"  catalog            = {catalog}\")\n",
        "print(f\"  bronze_schema      = {bronze_schema}\")\n",
        "print(f\"  source_table_full  = {source_table_full}\")\n",
        "print(f\"  raw_table_full     = {raw_table_full}\")\n",
        "print(f\"  filter_tags        = {filter_tags}\")\n",
        "print(f\"  filter_active      = {filter_active}\")\n",
        "print(f\"  filter_status      = {filter_status}\")\n",
        "print(f\"  file_extensions    = {file_extensions}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Runtime widgets (override mode/tags if needed)\n",
        "\n",
        "dbutils.widgets.dropdown(\"write_mode\", \"append\", [\"overwrite\", \"append\"], \"Write mode\")\n",
        "dbutils.widgets.text(\"tag_filter_override\", \"\", \"Tag filter override (comma-separated, optional)\")\n",
        "\n",
        "write_mode          = dbutils.widgets.get(\"write_mode\")\n",
        "tag_filter_override = dbutils.widgets.get(\"tag_filter_override\").strip()\n",
        "\n",
        "if tag_filter_override:\n",
        "    override_tags = [t.strip() for t in tag_filter_override.split(\",\") if t.strip()]\n",
        "    if override_tags:\n",
        "        print(f\"\\nOverriding config filter_tags with: {override_tags}\")\n",
        "        filter_tags = override_tags\n",
        "\n",
        "print(f\"\\nRuntime settings:\")\n",
        "print(f\"  write_mode         = {write_mode}\")\n",
        "print(f\"  effective_tags     = {filter_tags}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Utility: catalog & schema\n",
        "\n",
        "def use_catalog_and_schema(catalog: str, schema: str):\n",
        "    spark.sql(f\"USE CATALOG `{catalog}`\")\n",
        "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS `{schema}`\")\n",
        "    spark.sql(f\"USE SCHEMA `{schema}`\")\n",
        "\n",
        "use_catalog_and_schema(catalog, bronze_schema)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Generic function: md_dta_history → md_oa_file_raw\n",
        "\n",
        "def process_history_to_oa_raw(\n",
        "    catalog: str,\n",
        "    schema: str,\n",
        "    source_table_full: str,\n",
        "    raw_table_full: str,\n",
        "    filter_tags: list,\n",
        "    file_extensions: list,\n",
        "    filter_active: bool = True,\n",
        "    filter_status: str | None = None,\n",
        "    parse_version: str = \"2.0\",\n",
        "    write_mode: str = \"append\",\n",
        "):\n",
        "    \"\"\"\n",
        "    1) Reads candidate docs from source_table_full (e.g. md_dta_history).\n",
        "    2) Applies generic filters:\n",
        "       - file_extension IN file_extensions\n",
        "       - document_tags contains ANY of filter_tags (if provided)\n",
        "       - active == True (optional)\n",
        "       - is_current == True (fixed rule for history)\n",
        "       - status == filter_status (if provided)\n",
        "    3) Uses ai_parse_document(unbase64(content_base64), map('version', parse_version)).\n",
        "    4) Builds cleaned 'content' string from parsed.document.elements.\n",
        "    5) Writes to raw_table_full using save_with_audit.\n",
        "    \"\"\"\n",
        "\n",
        "    use_catalog_and_schema(catalog, schema)\n",
        "\n",
        "    print(f\"\\n[process_history_to_oa_raw] Reading from: {source_table_full}\")\n",
        "\n",
        "    df = spark.table(source_table_full)\n",
        "\n",
        "    # --- Build filters generically ---\n",
        "    conds = []\n",
        "\n",
        "    # File extensions\n",
        "    if file_extensions:\n",
        "        conds.append(F.col(\"file_extension\").isin(file_extensions))\n",
        "\n",
        "    # Tags: ANY of filter_tags in document_tags\n",
        "    if filter_tags:\n",
        "        tag_conds = [F.array_contains(F.col(\"document_tags\"), t) for t in filter_tags]\n",
        "        tag_expr = None\n",
        "        for c in tag_conds:\n",
        "            tag_expr = c if tag_expr is None else (tag_expr | c)\n",
        "        conds.append(tag_expr)\n",
        "\n",
        "    # Active flag\n",
        "    if filter_active:\n",
        "        conds.append(F.col(\"active\") == True)\n",
        "\n",
        "    # Always current\n",
        "    conds.append(F.col(\"is_current\") == True)\n",
        "\n",
        "    # Status filter\n",
        "    if filter_status:\n",
        "        conds.append(F.col(\"status\") == filter_status)\n",
        "\n",
        "    # Apply all conditions\n",
        "    from functools import reduce\n",
        "    import operator\n",
        "\n",
        "    if conds:\n",
        "        combined = reduce(operator.and_, conds)\n",
        "        df_raw = df.filter(combined)\n",
        "    else:\n",
        "        df_raw = df\n",
        "\n",
        "    src_count = df_raw.count()\n",
        "    print(f\"[process_history_to_oa_raw] Filtered history rows: {src_count}\")\n",
        "\n",
        "    if src_count == 0:\n",
        "        print(\"[process_history_to_oa_raw] No matching rows to process; exiting early.\")\n",
        "        dbutils.jobs.taskValues.set(key=\"oa_parse_status\", value=\"NO_DATA\")\n",
        "        dbutils.jobs.taskValues.set(key=\"oa_parse_count\", value=\"0\")\n",
        "        return\n",
        "\n",
        "    # --- Parsing ---\n",
        "    # Note: file_name, file_extension, file_version removed - available in md_file_history\n",
        "    df_parsed = (\n",
        "        df_raw\n",
        "        .withColumn(\n",
        "            \"parsed\",\n",
        "            F.expr(\n",
        "                f\"ai_parse_document(\"\n",
        "                f\"  unbase64(content_base64), \"\n",
        "                f\"  map('version','{parse_version}')\"\n",
        "                f\")\"\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # --- Build 'content' from elements (generic, checkbox-aware) ---\n",
        "    # Parent Document ID Logic:\n",
        "    # - For files extracted from a ZIP: parent_document_id = ZIP's document_id (use it)\n",
        "    # - For standalone uploads: parent_document_id = NULL, use document_id as fallback\n",
        "    df_clean = (\n",
        "        df_parsed\n",
        "        .select(\n",
        "            \"document_id\",\n",
        "            # Use parent_document_id if from ZIP, else fallback to document_id\n",
        "            F.coalesce(F.col(\"parent_document_id\"), F.col(\"document_id\")).alias(\"parent_document_id\"),\n",
        "            \"extracted_path\",\n",
        "            F.expr(\n",
        "                \"\"\"\n",
        "                concat_ws(\n",
        "                  '\\\\n\\\\n',\n",
        "                  transform(\n",
        "                    try_cast(parsed:document:elements AS ARRAY<VARIANT>),\n",
        "                    e ->\n",
        "                      CASE\n",
        "                        -- Checkbox-like element\n",
        "                        WHEN lower(try_cast(e:type AS STRING)) = 'checkbox' THEN\n",
        "                          (CASE\n",
        "                             WHEN coalesce(try_cast(e:checked AS BOOLEAN), false) THEN '☒ '\n",
        "                             ELSE '□ '\n",
        "                           END)\n",
        "                          ||\n",
        "                          coalesce(\n",
        "                            try_cast(e:label   AS STRING),\n",
        "                            try_cast(e:text    AS STRING),\n",
        "                            try_cast(e:content AS STRING),\n",
        "                            ''\n",
        "                          )\n",
        "\n",
        "                        -- Fallback: normal text element\n",
        "                        ELSE coalesce(\n",
        "                               try_cast(e:content AS STRING),\n",
        "                               try_cast(e:text    AS STRING),\n",
        "                               ''\n",
        "                             )\n",
        "                      END\n",
        "                  )\n",
        "                )\n",
        "                \"\"\"\n",
        "            ).alias(\"content\")\n",
        "        )\n",
        "        .where(F.col(\"content\").isNotNull() & (F.col(\"content\") != \"\"))\n",
        "    )\n",
        "\n",
        "    clean_count = df_clean.count()\n",
        "    print(f\"[process_history_to_oa_raw] Clean rows: {clean_count}\")\n",
        "\n",
        "    if clean_count == 0:\n",
        "        print(\"[process_history_to_oa_raw] No non-empty parsed content rows; nothing to write.\")\n",
        "        dbutils.jobs.taskValues.set(key=\"oa_parse_status\", value=\"NO_DATA\")\n",
        "        dbutils.jobs.taskValues.set(key=\"oa_parse_count\", value=\"0\")\n",
        "        return\n",
        "\n",
        "    # --- WRITE using save_with_audit ---\n",
        "    save_with_audit(\n",
        "        df=df_clean,\n",
        "        table_name=raw_table_full,\n",
        "        created_by_principal=created_by_principal,\n",
        "        databricks_job_id=databricks_job_id,\n",
        "        databricks_job_name=databricks_job_name,\n",
        "        databricks_run_id=databricks_run_id,\n",
        "        mode=write_mode,\n",
        "    )\n",
        "\n",
        "    print(\n",
        "        f\"[process_history_to_oa_raw] Saved {clean_count} row(s) to \"\n",
        "        f\"{catalog}.{raw_table_full} with save_with_audit (mode={write_mode})\"\n",
        "    )\n",
        "    \n",
        "    # Set task values for downstream\n",
        "    dbutils.jobs.taskValues.set(key=\"oa_parse_status\", value=\"SUCCESS\")\n",
        "    dbutils.jobs.taskValues.set(key=\"oa_parse_count\", value=str(clean_count))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: MAIN - run parse-only step\n",
        "\n",
        "process_history_to_oa_raw(\n",
        "    catalog=catalog,\n",
        "    schema=bronze_schema,\n",
        "    source_table_full=source_table_full,\n",
        "    raw_table_full=raw_table_full,\n",
        "    filter_tags=filter_tags,\n",
        "    file_extensions=file_extensions,\n",
        "    filter_active=filter_active,\n",
        "    filter_status=filter_status,\n",
        "    parse_version=\"2.0\",\n",
        "    write_mode=write_mode,\n",
        ")\n",
        "\n",
        "print(\"\\n✅ Operational Agreement history → md_oa_file_raw pipeline completed (parse-only, with audits).\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
