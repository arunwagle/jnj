{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "6a52efac-2d10-41f3-808a-ab4e073b018f",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "source": [
        "# Generic File Processor with Autoloader\n",
        "# Handles both ZIP archives (extract contents) and regular documents (direct catalog)\n",
        "\n",
        "This notebook:\n",
        "\n",
        "- Uses **Auto Loader** to ingest files from a Unity Catalog Volume.\n",
        "- Routes files based on extension:\n",
        "  - **Archives** (.zip, .tar.gz, etc.) â†’ Extract contents recursively\n",
        "  - **Regular files** (.pdf, .xlsx, etc.) â†’ Catalog directly\n",
        "- Writes a **Delta manifest table** listing all processed files with metadata.\n",
        "- Supports configurable source/target paths via workflow parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "2f6e851e-f322-4ce8-bcfe-7affcc6507e7",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'source_root_dbfs': 'dbfs:/Volumes/aira/bronze_md/input_data/historical_data/zip',\n",
              " 'target_root_dbfs': 'dbfs:/Volumes/aira/bronze_md/input_data/historical_data/extracted',\n",
              " 'checkpoint_path': '/Volumes/aira/bronze_md/input_data/jobs/_checkpoints/zip_processor',\n",
              " 'manifest_table': 'aira.bronze_md.trial_data_history'}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import json\n",
        "from clinical_data_standards_framework.utils import (\n",
        "    create_binary_autoloader_stream,\n",
        "    create_zip_batch_processor,\n",
        "    save_with_audit,\n",
        "    get_manifest_schema,\n",
        "    \n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get configuration from setup task\n",
        "globals_dict = json.loads(dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"globals\"))\n",
        "services_dict = json.loads(dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"services\"))\n",
        "pipeline_config = json.loads(dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"pipeline_config\"))\n",
        "created_by_principal = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"created_by_principal\")\n",
        "databricks_job_id = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"databricks_job_id\")\n",
        "databricks_job_name = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"databricks_job_name\")\n",
        "databricks_run_id = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"databricks_run_id\")\n",
        "\n",
        "# Get document_type override (if provided from UI upload)\n",
        "# When set, this will be used as the primary document_tag instead of path-based extraction\n",
        "try:\n",
        "    document_type_override = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"document_type\")\n",
        "    if document_type_override and document_type_override.strip():\n",
        "        print(f\"ðŸ“„ Document type override from UI: {document_type_override}\")\n",
        "    else:\n",
        "        document_type_override = None\n",
        "except Exception:\n",
        "    document_type_override = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract needed values from globals\n",
        "catalog = globals_dict['catalog']\n",
        "schema = globals_dict['bronze_schema']\n",
        "\n",
        "# Get file processor configuration from pipeline_config\n",
        "# New flatter structure - no more nested archives/clinical_trial_data/extraction\n",
        "manifest_table = pipeline_config['manifest_table']\n",
        "manifest_table_description = pipeline_config.get('manifest_table_description', '')\n",
        "checkpoint_subdir = pipeline_config['checkpoint_subdir']\n",
        "supported_extensions = pipeline_config['supported_extensions']\n",
        "\n",
        "# Get document categorization config\n",
        "document_categories = pipeline_config.get('document_categories', {})\n",
        "\n",
        "# Get file type definitions (for routing logic)\n",
        "file_types = pipeline_config.get('file_types', {})\n",
        "archive_extensions = file_types.get('archives', {}).get('extensions', ['.zip'])\n",
        "document_extensions = file_types.get('documents', {}).get('extensions', ['.pdf', '.xlsx'])\n",
        "\n",
        "# Get streaming configuration for scalability\n",
        "# max_files_per_trigger controls how many files are processed per micro-batch\n",
        "# Recommended: 10-50 for ZIP files, 100-500 for small documents\n",
        "streaming_config = pipeline_config.get('streaming', {})\n",
        "max_files_per_trigger = streaming_config.get('max_files_per_trigger', 20)  # Default: 20 for ZIPs\n",
        "max_bytes_per_trigger = streaming_config.get('max_bytes_per_trigger', None)  # Optional: e.g., \"256MB\"\n",
        "\n",
        "print(f\"\\nðŸ“‹ File Processor Configuration:\")\n",
        "print(f\"  Catalog: {catalog}\")\n",
        "print(f\"  Schema: {schema}\")\n",
        "print(f\"  Manifest Table: {manifest_table}\")\n",
        "print(f\"  Archive extensions: {archive_extensions}\")\n",
        "print(f\"  Document extensions: {document_extensions}\")\n",
        "print(f\"  Supported extensions (from extracted): {supported_extensions}\")\n",
        "print(f\"  Document categories: {len(document_categories)} configured\")\n",
        "print(f\"\\nâš¡ Streaming Configuration (Scalability):\")\n",
        "print(f\"  Max files per trigger: {max_files_per_trigger}\")\n",
        "print(f\"  Max bytes per trigger: {max_bytes_per_trigger or 'Auto'}\")\n",
        "print(f\"\\nâœ… Config loaded - paths will be computed from setup task values\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read source/target paths from setup task (computed from workflow params or config defaults)\n",
        "source_volume = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"source_volume\")\n",
        "source_subdir = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"source_subdir\")\n",
        "target_volume = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"target_volume\")\n",
        "target_subdir = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"target_subdir\")\n",
        "\n",
        "# Construct full paths\n",
        "source_root = f\"/Volumes/{catalog}/{schema}/{source_volume}/{source_subdir}\"\n",
        "target_root = f\"/Volumes/{catalog}/{schema}/{target_volume}/{target_subdir}\"\n",
        "manifest_table_full_name = f\"{catalog}.{schema}.{manifest_table}\"\n",
        "\n",
        "# Create source-specific checkpoint path to avoid conflicts when processing different sources\n",
        "# Replace slashes with underscores to create a safe directory name\n",
        "source_identifier = source_subdir.replace('/', '_')\n",
        "checkpoint_path = f\"/Volumes/{catalog}/{schema}/{source_volume}/{checkpoint_subdir}_{source_identifier}\"\n",
        "\n",
        "print(f\"\\nðŸ“ File Processor Paths:\")\n",
        "print(f\"  Source root: {source_root}\")\n",
        "print(f\"  Target root: {target_root}\")\n",
        "print(f\"  Manifest table: {manifest_table_full_name}\")\n",
        "print(f\"  Checkpoint: {checkpoint_path}\")\n",
        "print(f\"    (unique for source: {source_subdir})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "59fb668a-37f1-4dd8-8869-a60700622339",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- path: string (nullable = true)\n",
            " |-- modificationTime: timestamp (nullable = true)\n",
            " |-- length: long (nullable = true)\n",
            " |-- content: binary (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create Auto Loader streaming DataFrame for all files\n",
        "# Reads both archives (.zip, .tar.gz, etc.) and regular files (.pdf, .xlsx, etc.)\n",
        "# Uses max_files_per_trigger for controlled batching (scalability for millions of files)\n",
        "file_stream_df = create_binary_autoloader_stream(\n",
        "    spark=spark,\n",
        "    source_path=source_root,\n",
        "    max_files_per_trigger=max_files_per_trigger,\n",
        "    max_bytes_per_trigger=max_bytes_per_trigger\n",
        ")\n",
        "\n",
        "print(f\"âœ… Auto Loader stream created with controlled batching\")\n",
        "print(f\"   - Each micro-batch processes up to {max_files_per_trigger} files\")\n",
        "print(f\"   - Each ZIP gets its own parent_document_id (fixed in v2)\")\n",
        "\n",
        "# Display schema\n",
        "file_stream_df.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "678d9d5f-254b-42c7-9790-9cc28ede1f5d",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Create generic file batch processor with routing logic\n",
        "# Routes files based on extension: ZIP â†’ extract, regular â†’ catalog\n",
        "\n",
        "# Create ZIP-specific processor\n",
        "# Pass document_type_override to set tags for all extracted files when set from UI\n",
        "zip_batch_processor = create_zip_batch_processor(\n",
        "    spark=spark,\n",
        "    target_root_path=target_root,\n",
        "    manifest_table_full_name=manifest_table_full_name,\n",
        "    created_by_principal=created_by_principal,\n",
        "    databricks_job_id=databricks_job_id,\n",
        "    databricks_job_name=databricks_job_name,\n",
        "    databricks_run_id=databricks_run_id,\n",
        "    supported_extensions=supported_extensions,\n",
        "    categories_config=document_categories,\n",
        "    manifest_table_description=manifest_table_description,\n",
        "    document_type_override=document_type_override,\n",
        ")\n",
        "\n",
        "# Wrapper function for file routing\n",
        "def process_file_batch(batch_df, batch_id):\n",
        "    \"\"\"Route files based on extension: archives vs regular documents\"\"\"\n",
        "    import os\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Processing batch {batch_id}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # Collect file paths and classify\n",
        "    files = batch_df.select(\"path\").collect()\n",
        "    archive_files = []\n",
        "    regular_files = []\n",
        "    \n",
        "    for file_row in files:\n",
        "        file_path = file_row.path\n",
        "        ext = os.path.splitext(file_path.lower())[1]\n",
        "        \n",
        "        if ext in archive_extensions:\n",
        "            archive_files.append(file_path)\n",
        "        elif ext in document_extensions:\n",
        "            regular_files.append(file_path)\n",
        "        else:\n",
        "            print(f\"  âš  Skipping unsupported file type: {file_path} (ext: {ext})\")\n",
        "    \n",
        "    print(f\"Files in batch: {len(files)}\")\n",
        "    print(f\"  Archives (.zip, etc.): {len(archive_files)}\")\n",
        "    print(f\"  Documents (.pdf, .xlsx, etc.): {len(regular_files)}\")\n",
        "    \n",
        "    # Process archives (if any)\n",
        "    if archive_files:\n",
        "        print(f\"\\nâ†’ Processing {len(archive_files)} archive file(s)...\")\n",
        "        archive_df = batch_df.filter(batch_df.path.isin(archive_files))\n",
        "        zip_batch_processor(archive_df, batch_id)\n",
        "    \n",
        "    # Process regular files (if any)\n",
        "    if regular_files:\n",
        "        print(f\"\\nâ†’ Processing {len(regular_files)} regular file(s)...\")\n",
        "        regular_df = batch_df.filter(batch_df.path.isin(regular_files))\n",
        "        process_regular_files(regular_df, batch_id)\n",
        "    \n",
        "    print(f\"âœ“ Batch {batch_id} complete\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "print(\"Generic file batch processor created successfully\")\n",
        "print(f\"Archive extensions: {archive_extensions}\")\n",
        "print(f\"Document extensions: {document_extensions}\")\n",
        "print(f\"Supported file extensions (from extracted contents): {supported_extensions}\")\n",
        "if document_categories:\n",
        "    print(f\"Document categorization enabled with {len(document_categories)} top-level categories\")\n",
        "else:\n",
        "    print(\"Document categorization not configured\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7: Regular File Processing Function\n",
        "# Handles non-archive files (PDF, Excel, etc.) using save_document_with_metadata with versioning\n",
        "# Uses save_with_audit for automatic audit column management\n",
        "# NOTE: Status is now consolidated in md_file_history - no separate status table\n",
        "\n",
        "def process_regular_files(regular_df, batch_id):\n",
        "    \"\"\"\n",
        "    Process regular files (non-archives) using the same pattern as ZIP extraction.\n",
        "    \n",
        "    Supports versioning: if the same file is uploaded multiple times:\n",
        "    - Old version is marked as is_current=false\n",
        "    - New version gets incremented version number and supersedes_document_id link\n",
        "    \n",
        "    Status tracking is consolidated in md_file_history:\n",
        "    - status field tracks processing status (READY_FOR_PROCESSING, COMPLETED, ERROR)\n",
        "    - processing_duration_seconds, error_message, retry_count for processing details\n",
        "    \n",
        "    NOTE: Audit columns are added automatically by save_with_audit() - \n",
        "          DO NOT add them manually to records!\n",
        "    \"\"\"\n",
        "    import os\n",
        "    from datetime import datetime\n",
        "    from clinical_data_standards_framework.utils import (\n",
        "        save_document_with_metadata,\n",
        "        save_with_audit,\n",
        "        get_manifest_schema,\n",
        "    )\n",
        "    \n",
        "    # Collect file info WITH binary content (already loaded by Autoloader)\n",
        "    files = regular_df.select(\"path\", \"modificationTime\", \"length\", \"content\").collect()\n",
        "    \n",
        "    if not files:\n",
        "        print(\"  No regular files to process\")\n",
        "        return\n",
        "    \n",
        "    # Process each file using the unified function\n",
        "    manifest_records = []\n",
        "    old_version_ids = []  # Track old versions to mark as not current\n",
        "    error_count = 0\n",
        "    \n",
        "    for file_row in files:\n",
        "        source_path = file_row.path\n",
        "        content_bytes = file_row.content  # âœ… Binary content already loaded by Autoloader!\n",
        "        file_size = file_row.length\n",
        "        file_name = os.path.basename(source_path)\n",
        "        \n",
        "        # Compute target path (maintain same folder structure)\n",
        "        source_path_clean = source_path.replace('dbfs:', '')\n",
        "        relative_path = source_path_clean.replace(source_root, '').lstrip('/')\n",
        "        target_path = f\"{target_root}/{relative_path}\"\n",
        "        \n",
        "        try:\n",
        "            # Use the unified save_document_with_metadata function\n",
        "            # This handles file write, categorization, base64 encoding, versioning, and metadata creation\n",
        "            # NOTE: Returns feature columns only - audit columns added by save_with_audit\n",
        "            # Status fields (status, processing_duration_seconds, etc.) are included in the manifest record\n",
        "            manifest_record = save_document_with_metadata(\n",
        "                content_bytes=content_bytes,\n",
        "                source_path=source_path,\n",
        "                target_path=target_path,\n",
        "                file_size=file_size,\n",
        "                archive_root=target_root,\n",
        "                categories_config=document_categories,\n",
        "                created_by_principal=created_by_principal,\n",
        "                databricks_job_id=databricks_job_id,\n",
        "                databricks_job_name=databricks_job_name,\n",
        "                databricks_run_id=databricks_run_id,\n",
        "                is_from_nested_zip=False,\n",
        "                nested_level=0,\n",
        "                spark=spark,  # Pass spark for version checking\n",
        "                manifest_table_full_name=manifest_table_full_name\n",
        "            )\n",
        "            \n",
        "            # Override document_tags and active flag if document_type was set from UI\n",
        "            # This takes precedence over path-based tag extraction\n",
        "            if document_type_override:\n",
        "                manifest_record['document_tags'] = [document_type_override]\n",
        "                manifest_record['active'] = True  # UI-uploaded files should be active\n",
        "                print(f\"  ðŸ“„ Tags set from UI: {manifest_record['document_tags']} (active=True)\")\n",
        "            \n",
        "            manifest_records.append(manifest_record)\n",
        "            \n",
        "            # Track if this is an update (has supersedes_document_id)\n",
        "            if manifest_record.get('supersedes_document_id'):\n",
        "                old_version_ids.append(manifest_record['supersedes_document_id'])\n",
        "                print(f\"  âœ“ Updated: {file_name} (v{manifest_record['version']})\")\n",
        "            else:\n",
        "                print(f\"  âœ“ Processed: {file_name} (v1)\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            error_count += 1\n",
        "            print(f\"  âœ— ERROR processing file {file_name}: {str(e)}\")\n",
        "            # Note: Errors without a document are logged but not saved to table\n",
        "            # (no document_id to associate with)\n",
        "    \n",
        "    # Write to Delta table using save_with_audit (adds audit columns automatically)\n",
        "    # Status is now part of the manifest record - no separate status table needed\n",
        "    if manifest_records:\n",
        "        manifest_df = spark.createDataFrame(manifest_records, schema=get_manifest_schema())\n",
        "        # Use save_with_audit - it adds audit columns automatically\n",
        "        save_with_audit(\n",
        "            df=manifest_df,\n",
        "            table_name=manifest_table_full_name,\n",
        "            created_by_principal=created_by_principal,\n",
        "            databricks_job_id=databricks_job_id,\n",
        "            databricks_job_name=databricks_job_name,\n",
        "            databricks_run_id=databricks_run_id,\n",
        "            mode='append'\n",
        "        )\n",
        "        print(f\"  âœ“ Saved {len(manifest_records)} document(s) to md_file_history\")\n",
        "        \n",
        "        # Mark old versions as not current (if any updates occurred)\n",
        "        if old_version_ids:\n",
        "            ids_list = ', '.join([f\"'{id}'\" for id in old_version_ids])\n",
        "            spark.sql(f\"\"\"\n",
        "                UPDATE {manifest_table_full_name}\n",
        "                SET is_current = false,\n",
        "                    last_updated_by_principal = '{created_by_principal}',\n",
        "                    last_updated_ts = current_timestamp()\n",
        "                WHERE document_id IN ({ids_list})\n",
        "            \"\"\")\n",
        "            print(f\"  âœ“ Marked {len(old_version_ids)} old version(s) as not current\")\n",
        "    \n",
        "    if error_count > 0:\n",
        "        print(f\"  âš  {error_count} file(s) had errors and were not saved\")\n",
        "\n",
        "print(\"âœ… Regular file processor function created (status consolidated in md_file_history)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "abb34f47-cccc-4bf7-82f5-90c734419500",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Start the Auto Loader streaming query\n",
        "# For one-time historical processing, use availableNow=True\n",
        "# For continuous ingestion, change trigger to processingTime or remove trigger entirely\n",
        "\n",
        "query = (\n",
        "    file_stream_df.writeStream\n",
        "                  .foreachBatch(process_file_batch)  # â† Generic processor (routes to ZIP or regular)\n",
        "                  .option('checkpointLocation', checkpoint_path)\n",
        "                  .trigger(availableNow=True)  # change for continuous streaming if desired\n",
        "                  .start()\n",
        ")\n",
        "\n",
        "query.awaitTermination()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "cellMetadata": {
            "byteLimit": 2048000,
            "rowLimit": 10000
          },
          "inputWidgets": {},
          "nuid": "7fb40779-1a3d-4331-81c3-07709028c923",
          "showTitle": false,
          "tableResultSettingsMap": {},
          "title": ""
        }
      },
      "outputs": [],
      "source": [
        "# Cell 8: Conditional OPTIMIZE - trial_data_history table\n",
        "# Only run OPTIMIZE when conditions are met to save compute resources\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "# Get table stats\n",
        "try:\n",
        "    table_stats = spark.sql(f\"DESCRIBE DETAIL {manifest_table_full_name}\").first()\n",
        "    num_files = table_stats.numFiles\n",
        "    total_size_gb = table_stats.sizeInBytes / (1024**3)\n",
        "    avg_file_size_mb = (total_size_gb * 1024) / num_files if num_files > 0 else 0\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"TABLE STATS: {manifest_table_full_name}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Total files: {num_files}\")\n",
        "    print(f\"Total size: {total_size_gb:.2f} GB\")\n",
        "    print(f\"Avg file size: {avg_file_size_mb:.2f} MB\")\n",
        "    \n",
        "    # Determine if OPTIMIZE is needed\n",
        "    should_optimize = False\n",
        "    reason = \"\"\n",
        "    \n",
        "    # We don't know exact doc count processed, so use file-based heuristics\n",
        "    if num_files > 200 and avg_file_size_mb < 100:  # Condition 1: Small file problem\n",
        "        should_optimize = True\n",
        "        reason = f\"Small file problem ({num_files} files, avg {avg_file_size_mb:.2f}MB)\"\n",
        "    elif num_files > 500:  # Condition 2: Too many files\n",
        "        should_optimize = True\n",
        "        reason = f\"Too many files ({num_files})\"\n",
        "    elif total_size_gb > 1 and num_files > 100 and avg_file_size_mb < 200:  # Condition 3: Growing dataset with small files\n",
        "        should_optimize = True\n",
        "        reason = f\"Dataset growing with sub-optimal file sizes ({total_size_gb:.2f}GB, {num_files} files)\"\n",
        "    \n",
        "    if should_optimize:\n",
        "        print(f\"\\nðŸ”§ OPTIMIZE TRIGGERED: {reason}\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "        start_time = datetime.now()\n",
        "        \n",
        "        # Run OPTIMIZE with Z-ORDER\n",
        "        spark.sql(f\"\"\"\n",
        "            OPTIMIZE {manifest_table_full_name}\n",
        "            ZORDER BY (status, document_tags)\n",
        "        \"\"\")\n",
        "        \n",
        "        # Get stats after optimization\n",
        "        table_stats_after = spark.sql(f\"DESCRIBE DETAIL {manifest_table_full_name}\").first()\n",
        "        num_files_after = table_stats_after.numFiles\n",
        "        \n",
        "        duration = (datetime.now() - start_time).total_seconds()\n",
        "        \n",
        "        print(f\"âœ“ OPTIMIZE completed in {duration:.1f} seconds\")\n",
        "        print(f\"  Files: {num_files} â†’ {num_files_after} ({num_files - num_files_after} merged)\")\n",
        "        print(f\"{'='*80}\")\n",
        "    else:\n",
        "        print(f\"\\nâ­ï¸  OPTIMIZE SKIPPED: Conditions not met\")\n",
        "        print(f\"  - Files: {num_files} (threshold: >200 with small files OR >500 total)\")\n",
        "        print(f\"  - Avg file size: {avg_file_size_mb:.2f}MB (threshold: <100-200MB)\")\n",
        "        print(f\"  - Total size: {total_size_gb:.2f}GB\")\n",
        "        print(f\"{'='*80}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"\\nâš ï¸  Could not check OPTIMIZE conditions: {str(e)}\")\n",
        "    print(\"Continuing without optimization...\")"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": null,
      "dashboards": [],
      "environmentMetadata": null,
      "inputWidgetPreferences": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "notebookName": "nb_zip_processor",
      "widgets": {}
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
