{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approve DTA Version (Create DTA Major)\n",
    "\n",
    "Promotes the current draft from Silver to DTA Major in Gold library for ALL configured library types.\n",
    "\n",
    "**Operation:** `APPROVE_DTA` / `CREATE_DTA_APPROVED`\n",
    "\n",
    "**Modes:**\n",
    "- `HISTORICAL`: Promotes draft to DTA Major (auto-approval for historical imports)\n",
    "- `UI`: Promotes draft to DTA Major (triggered by workflow approval)\n",
    "\n",
    "**What it does:**\n",
    "1. Gets DTA ID(s) from widget or task values\n",
    "2. Iterates over ALL configured library types (transfer_variables, test_concepts, etc.)\n",
    "3. For each library type with records:\n",
    "   - Reads draft records from Silver\n",
    "   - Creates DTA Major version in Gold library (copies records with `version` = major version)\n",
    "   - Updates Silver records: `version_status=APPROVED`\n",
    "   - Registers DTA_APPROVED version in registry\n",
    "4. Updates DTA entity: `version`, `status=ACTIVE` (aligns 1:1 with md_version_registry.status)\n",
    "\n",
    "**Version handling:**\n",
    "- **Silver**: Keeps draft version tag (e.g., `1.0-DTA001-draft1`) with `version_status=APPROVED`\n",
    "- **Gold**: Gets major version in `version` column (e.g., `1.0-DTA001-v1.0`)\n",
    "\n",
    "**Supported Library Types:**\n",
    "- `transfer_variables` - md_dta_transfer_variables\n",
    "- `test_concepts` - md_dta_vendor_test_concepts\n",
    "\n",
    "**Parameters:**\n",
    "- `dta_id` - Single DTA identifier (for UI/API call)\n",
    "- `source` - HISTORICAL or UI\n",
    "- `library_type` - Optional filter (empty = process ALL library types)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from clinical_data_standards_framework.versioning import get_version_registry_schema\n",
    "from clinical_data_standards_framework.utils import log_dta_version_event, log_dta_workflow_event, compute_definition_hash_spark\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"APPROVE DTA VERSION (Create DTA Major)\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Get Configuration from Setup Task\n",
    "globals_dict = json.loads(dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"globals\"))\n",
    "versioning_dict = json.loads(dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"versioning\"))\n",
    "created_by_principal = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"created_by_principal\")\n",
    "databricks_job_id = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"databricks_job_id\")\n",
    "databricks_job_name = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"databricks_job_name\")\n",
    "databricks_run_id = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"databricks_run_id\")\n",
    "\n",
    "catalog = globals_dict['catalog']\n",
    "silver_schema = globals_dict['silver_schema']\n",
    "gold_schema = globals_dict['gold_schema']\n",
    "\n",
    "# Versioning configuration from YAML\n",
    "library_tables = versioning_dict.get('library_tables', [])\n",
    "registry_table_name = versioning_dict.get('registry_table', 'md_version_registry')\n",
    "\n",
    "# Definition hash configuration (all from versioning section)\n",
    "# Common fields are always included in ALL library type hashes\n",
    "definition_hash_common_fields = versioning_dict.get('definition_hash_common_fields', ['data_provider_name', 'data_stream_type'])\n",
    "\n",
    "# Per-library type hash fields (from versioning section, not pipeline_config)\n",
    "definition_hash_fields_config = versioning_dict.get('definition_hash_fields_by_type', {})\n",
    "definition_hash_fields_by_type = {}\n",
    "for lib_type in definition_hash_fields_config:\n",
    "    definition_hash_fields_by_type[lib_type] = definition_hash_fields_config.get(lib_type, [])\n",
    "\n",
    "# Build gold_columns map from config (no more hardcoded values)\n",
    "# Format: {library_type: [column_names]}\n",
    "gold_columns_by_type = {}\n",
    "for lib_config in library_tables:\n",
    "    lib_type = lib_config.get('library_type')\n",
    "    gold_cols = lib_config.get('gold_columns', [])\n",
    "    if lib_type:\n",
    "        gold_columns_by_type[lib_type] = gold_cols\n",
    "\n",
    "# Build related_tables map from config\n",
    "# Format: {library_type: [{silver_table, gold_table, gold_columns}, ...]}\n",
    "related_tables_by_type = {}\n",
    "for lib_config in library_tables:\n",
    "    lib_type = lib_config.get('library_type')\n",
    "    related_tables = lib_config.get('related_tables', [])\n",
    "    if lib_type and related_tables:\n",
    "        related_tables_by_type[lib_type] = related_tables\n",
    "\n",
    "print(f\"Catalog: {catalog}\")\n",
    "print(f\"Silver Schema: {silver_schema}\")\n",
    "print(f\"Gold Schema: {gold_schema}\")\n",
    "print(f\"Created by: {created_by_principal}\")\n",
    "print(f\"Definition hash common fields: {definition_hash_common_fields}\")\n",
    "print(f\"Definition hash fields by type: {list(definition_hash_fields_by_type.keys())}\")\n",
    "print(f\"Gold columns configured for: {list(gold_columns_by_type.keys())}\")\n",
    "print(f\"Related tables configured for: {list(related_tables_by_type.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Get Parameters from Task Values (set by create_dta_instance)\n",
    "# This notebook runs as part of job_cdm_dta_create after save_draft\n",
    "\n",
    "# Get DTA IDs and parameters from create_dta_instance task values\n",
    "try:\n",
    "    dta_ids = json.loads(dbutils.jobs.taskValues.get(taskKey=\"create_dta_instance\", key=\"created_dta_ids\"))\n",
    "    source = dbutils.jobs.taskValues.get(taskKey=\"create_dta_instance\", key=\"source\")\n",
    "    library_type_param = dbutils.jobs.taskValues.get(taskKey=\"create_dta_instance\", key=\"library_type\")\n",
    "    print(f\"Mode: Reading from create_dta_instance task values\")\n",
    "except Exception as e:\n",
    "    raise ValueError(\n",
    "        f\"Could not get task values from create_dta_instance: {e}. \"\n",
    "        \"This notebook must run after create_dta_instance task in the job pipeline.\"\n",
    "    )\n",
    "\n",
    "# Validate we have at least one DTA ID\n",
    "if not dta_ids:\n",
    "    raise ValueError(\"No DTAs created by create_dta_instance task\")\n",
    "\n",
    "print(f\"Source: {source}\")\n",
    "print(f\"DTAs to process: {len(dta_ids)}\")\n",
    "print(f\"DTA IDs: {dta_ids}\")\n",
    "\n",
    "# Determine which library types to process\n",
    "# If library_type is specified, only process that type\n",
    "# If empty, process ALL configured library types\n",
    "if library_type_param:\n",
    "    library_types_to_process = [lib for lib in library_tables if lib.get('library_type') == library_type_param]\n",
    "    if not library_types_to_process:\n",
    "        raise ValueError(\n",
    "            f\"library_type '{library_type_param}' not found in config. \"\n",
    "            f\"Available types: {[lib.get('library_type') for lib in library_tables]}\"\n",
    "        )\n",
    "else:\n",
    "    # Process ALL configured library types\n",
    "    if not library_tables:\n",
    "        raise ValueError(\n",
    "            \"No library_tables configured in versioning config. \"\n",
    "            \"Check that md_config_cache is populated correctly.\"\n",
    "        )\n",
    "    library_types_to_process = library_tables\n",
    "\n",
    "print(f\"Library types to process: {[lib.get('library_type') for lib in library_types_to_process]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Configure Table Names\n",
    "dta_table = f\"{catalog}.{gold_schema}.dta\"\n",
    "registry_table = f\"{catalog}.{gold_schema}.{registry_table_name}\"\n",
    "\n",
    "# Build table maps from config\n",
    "silver_table_map = {}\n",
    "library_table_map = {}\n",
    "\n",
    "for lib_config in library_types_to_process:\n",
    "    lib_type = lib_config.get('library_type')\n",
    "    \n",
    "    # Silver table\n",
    "    silver_name = lib_config.get('silver_table', 'md_dta_transfer_variables_draft')\n",
    "    silver_sch = lib_config.get('silver_schema', silver_schema)\n",
    "    silver_table_map[lib_type] = f\"{catalog}.{silver_sch}.{silver_name}\"\n",
    "    \n",
    "    # Gold library table\n",
    "    lib_name = lib_config.get('name')\n",
    "    lib_schema = lib_config.get('schema', gold_schema)\n",
    "    if lib_type and lib_name:\n",
    "        library_table_map[lib_type] = f\"{catalog}.{lib_schema}.{lib_name}\"\n",
    "\n",
    "print(f\"\\nDTA Table: {dta_table}\")\n",
    "print(f\"Registry Table: {registry_table}\")\n",
    "print(f\"\\nSilver Tables:\")\n",
    "for lib_type, table in silver_table_map.items():\n",
    "    print(f\"  {lib_type}: {table}\")\n",
    "print(f\"\\nGold Library Tables:\")\n",
    "for lib_type, table in library_table_map.items():\n",
    "    print(f\"  {lib_type}: {table}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Load DTA Metadata\n",
    "now = datetime.now()\n",
    "base_template_version = \"1.0\"\n",
    "\n",
    "df_dta = spark.table(dta_table).filter(F.col(\"dta_id\").isin(dta_ids))\n",
    "dta_metadata = {row[\"dta_id\"]: row.asDict() for row in df_dta.collect()}\n",
    "print(f\"\\nLoaded metadata for {len(dta_metadata)} DTAs\")\n",
    "\n",
    "# Get draft version tags from save_draft or DTA metadata\n",
    "try:\n",
    "    draft_versions = json.loads(dbutils.jobs.taskValues.get(taskKey=\"save_draft\", key=\"draft_versions\"))\n",
    "except:\n",
    "    draft_versions = {dta_id: dta_metadata.get(dta_id, {}).get(\"current_draft_version\") for dta_id in dta_ids}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Create DTA Major Versions - Update Silver and Prepare Major Versions\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Creating DTA Major versions (ALL library types)...\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "major_versions = {}\n",
    "# Structure: {dta_id: {library_type: {major_version, draft_count, gold_count}}}\n",
    "major_versions_by_dta = {}\n",
    "\n",
    "for dta_id in dta_ids:\n",
    "    meta = dta_metadata.get(dta_id, {})\n",
    "    dta_number = meta.get(\"dta_number\", \"DTA000\")\n",
    "    draft_version = draft_versions.get(dta_id, f\"{base_template_version}-{dta_number}-draft1\")\n",
    "    \n",
    "    # Create major version tag\n",
    "    major_version = f\"{base_template_version}-{dta_number}-v1.0\"\n",
    "    major_versions[dta_id] = major_version\n",
    "    major_versions_by_dta[dta_id] = {}\n",
    "    \n",
    "    print(f\"\\nProcessing DTA: {dta_number}\")\n",
    "    print(f\"  Draft: {draft_version} → Major: {major_version}\")\n",
    "    \n",
    "    # Process each library type\n",
    "    for library_type, silver_table in silver_table_map.items():\n",
    "        print(f\"\\n  [{library_type}]\")\n",
    "        \n",
    "        # Check if silver table exists\n",
    "        if not spark.catalog.tableExists(silver_table):\n",
    "            print(f\"    ⚠ Table does not exist, skipping\")\n",
    "            major_versions_by_dta[dta_id][library_type] = {\"draft_count\": 0, \"gold_count\": 0}\n",
    "            continue\n",
    "        \n",
    "        # Read draft records from Silver\n",
    "        df_draft = spark.table(silver_table).filter(\n",
    "            (F.col(\"dta_id\") == dta_id) & \n",
    "            (F.col(\"version_status\") == \"DRAFT\")\n",
    "        )\n",
    "        draft_count = df_draft.count()\n",
    "        print(f\"    Silver draft records: {draft_count}\")\n",
    "        \n",
    "        if draft_count == 0:\n",
    "            print(f\"    ⚠ No draft records found, skipping\")\n",
    "            major_versions_by_dta[dta_id][library_type] = {\"draft_count\": 0, \"gold_count\": 0}\n",
    "            continue\n",
    "        \n",
    "        # Update Silver: mark as APPROVED but KEEP the draft version\n",
    "        spark.sql(f\"\"\"\n",
    "            UPDATE {silver_table}\n",
    "            SET version_status = 'APPROVED',\n",
    "                is_current_draft = false\n",
    "            WHERE dta_id = '{dta_id}' AND version_status = 'DRAFT'\n",
    "        \"\"\")\n",
    "        print(f\"    ✓ Updated silver records to APPROVED\")\n",
    "        \n",
    "        # Also update related tables if configured (e.g., OA attributes, options, other)\n",
    "        related_tables = related_tables_by_type.get(library_type, [])\n",
    "        for rel_config in related_tables:\n",
    "            rel_silver_name = rel_config.get('silver_table')\n",
    "            if not rel_silver_name:\n",
    "                continue\n",
    "            \n",
    "            rel_silver_table = f\"{catalog}.{silver_schema}.{rel_silver_name}\"\n",
    "            \n",
    "            # Check if related silver table exists\n",
    "            if not spark.catalog.tableExists(rel_silver_table):\n",
    "                print(f\"    ⚠ Related table {rel_silver_name} does not exist, skipping update\")\n",
    "                continue\n",
    "            \n",
    "            # Update related table records to APPROVED\n",
    "            spark.sql(f\"\"\"\n",
    "                UPDATE {rel_silver_table}\n",
    "                SET version_status = 'APPROVED',\n",
    "                    is_current_draft = false\n",
    "                WHERE dta_id = '{dta_id}' AND version_status = 'DRAFT'\n",
    "            \"\"\")\n",
    "            print(f\"    ✓ Updated related table {rel_silver_name} to APPROVED\")\n",
    "        \n",
    "        # Store draft count for this library type\n",
    "        major_versions_by_dta[dta_id][library_type] = {\"draft_count\": draft_count, \"gold_count\": 0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Copy Records to Gold Library (All Library Types)\n",
    "# Computes definition_hash during promotion (silver has no hash column)\n",
    "# gold_columns_by_type is now loaded from config in Cell 2\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Copying DTA Major records to Gold library...\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Helper function to copy records from silver to gold\n",
    "def copy_to_gold_table(df_source, gold_columns, hash_fields, major_version, library_table, include_hash=True):\n",
    "    \"\"\"\n",
    "    Copy records from silver to gold table with version and optional definition_hash.\n",
    "    \n",
    "    Args:\n",
    "        df_source: Source DataFrame (filtered to approved records)\n",
    "        gold_columns: List of columns to copy from silver\n",
    "        hash_fields: List of fields for definition_hash computation\n",
    "        major_version: Version string for gold records\n",
    "        library_table: Target gold table name\n",
    "        include_hash: Whether to compute definition_hash (True for main tables, False for related)\n",
    "    \n",
    "    Returns:\n",
    "        Number of records written\n",
    "    \"\"\"\n",
    "    # Select columns that exist in the source table\n",
    "    available_cols = [c for c in gold_columns if c in df_source.columns]\n",
    "    \n",
    "    # Select and add gold-specific columns\n",
    "    df_gold = df_source.select(\n",
    "        *[F.col(c) for c in available_cols]\n",
    "    ).withColumn(\"version\", F.lit(major_version)\n",
    "    ).withColumn(\"is_major_version\", F.lit(True)\n",
    "    ).withColumn(\"is_dta_major\", F.lit(True)\n",
    "    ).withColumn(\"parent_version\", F.lit(base_template_version)\n",
    "    ).withColumn(\"effective_start_ts\", F.lit(now)\n",
    "    ).withColumn(\"effective_end_ts\", F.lit(None).cast(\"timestamp\")\n",
    "    ).withColumn(\"is_current\", F.lit(True))\n",
    "    \n",
    "    # Compute definition_hash for main library tables (related tables don't have hash)\n",
    "    if include_hash and hash_fields:\n",
    "        # Filter to columns that actually exist in the DataFrame\n",
    "        hash_fields_available = [f for f in hash_fields if f in df_gold.columns]\n",
    "        if hash_fields_available:\n",
    "            df_gold = compute_definition_hash_spark(df_gold, hash_fields_available, \"definition_hash\")\n",
    "            print(f\"      → Computed definition_hash using fields: {hash_fields_available}\")\n",
    "        else:\n",
    "            df_gold = df_gold.withColumn(\"definition_hash\", F.lit(None).cast(\"string\"))\n",
    "            print(f\"      ⚠ No hash fields available, definition_hash set to NULL\")\n",
    "    elif include_hash:\n",
    "        df_gold = df_gold.withColumn(\"definition_hash\", F.lit(None).cast(\"string\"))\n",
    "        print(f\"      ⚠ No hash fields configured, definition_hash set to NULL\")\n",
    "    \n",
    "    # Append to gold library with schema evolution\n",
    "    df_gold.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(library_table)\n",
    "    return df_gold.count()\n",
    "\n",
    "for dta_id in dta_ids:\n",
    "    meta = dta_metadata.get(dta_id, {})\n",
    "    major_version = major_versions[dta_id]\n",
    "    lib_type_data = major_versions_by_dta.get(dta_id, {})\n",
    "    \n",
    "    print(f\"\\n  DTA: {meta.get('dta_number')}\")\n",
    "    \n",
    "    # Copy to each library type\n",
    "    for library_type, silver_table in silver_table_map.items():\n",
    "        # Get library table for this type\n",
    "        library_table = library_table_map.get(library_type)\n",
    "        if not library_table:\n",
    "            print(f\"    [{library_type}] No gold table configured, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Check if we have records to copy\n",
    "        type_info = lib_type_data.get(library_type, {})\n",
    "        if type_info.get(\"draft_count\", 0) == 0:\n",
    "            print(f\"    [{library_type}] No records to copy, skipping\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"    [{library_type}]\")\n",
    "        \n",
    "        # Check if silver table exists\n",
    "        if not spark.catalog.tableExists(silver_table):\n",
    "            print(f\"      ⚠ Silver table does not exist, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Read approved records from Silver\n",
    "        df_approved = spark.table(silver_table).filter(\n",
    "            (F.col(\"dta_id\") == dta_id) & (F.col(\"version_status\") == \"APPROVED\")\n",
    "        )\n",
    "        \n",
    "        # Get columns for this library type from config\n",
    "        gold_columns = gold_columns_by_type.get(library_type, [])\n",
    "        if not gold_columns:\n",
    "            print(f\"      ⚠ No gold_columns configured for {library_type}, skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Get hash fields for this library type (type-specific + common fields)\n",
    "        type_hash_fields = definition_hash_fields_by_type.get(library_type, [])\n",
    "        all_hash_fields = type_hash_fields + definition_hash_common_fields\n",
    "        \n",
    "        # Copy main table using helper function\n",
    "        gold_count = copy_to_gold_table(\n",
    "            df_source=df_approved,\n",
    "            gold_columns=gold_columns,\n",
    "            hash_fields=all_hash_fields,\n",
    "            major_version=major_version,\n",
    "            library_table=library_table,\n",
    "            include_hash=True\n",
    "        )\n",
    "        \n",
    "        # Update tracking\n",
    "        major_versions_by_dta[dta_id][library_type][\"gold_count\"] = gold_count\n",
    "        print(f\"      ✓ Added {gold_count} records to {library_table}\")\n",
    "        \n",
    "        # Process related tables if configured (e.g., OA attributes, options, other)\n",
    "        related_tables = related_tables_by_type.get(library_type, [])\n",
    "        for rel_config in related_tables:\n",
    "            rel_silver_name = rel_config.get('silver_table')\n",
    "            rel_gold_name = rel_config.get('gold_table')\n",
    "            rel_gold_columns = rel_config.get('gold_columns', [])\n",
    "            \n",
    "            if not rel_silver_name or not rel_gold_name:\n",
    "                continue\n",
    "            \n",
    "            # Build full table names\n",
    "            rel_silver_table = f\"{catalog}.{silver_schema}.{rel_silver_name}\"\n",
    "            rel_gold_table = f\"{catalog}.{gold_schema}.{rel_gold_name}\"\n",
    "            \n",
    "            print(f\"      [{rel_silver_name} → {rel_gold_name}]\")\n",
    "            \n",
    "            # Check if silver table exists\n",
    "            if not spark.catalog.tableExists(rel_silver_table):\n",
    "                print(f\"        ⚠ Silver table does not exist, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Read approved records from silver related table\n",
    "            df_rel_approved = spark.table(rel_silver_table).filter(\n",
    "                (F.col(\"dta_id\") == dta_id) & (F.col(\"version_status\") == \"APPROVED\")\n",
    "            )\n",
    "            \n",
    "            rel_count = df_rel_approved.count()\n",
    "            if rel_count == 0:\n",
    "                print(f\"        ⚠ No approved records found, skipping\")\n",
    "                continue\n",
    "            \n",
    "            # Copy related table (no definition_hash for related tables)\n",
    "            rel_gold_count = copy_to_gold_table(\n",
    "                df_source=df_rel_approved,\n",
    "                gold_columns=rel_gold_columns,\n",
    "                hash_fields=[],  # Related tables don't have definition_hash\n",
    "                major_version=major_version,\n",
    "                library_table=rel_gold_table,\n",
    "                include_hash=False\n",
    "            )\n",
    "            print(f\"        ✓ Added {rel_gold_count} records to {rel_gold_table}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Register Versions and Update DTA Entities\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Registering versions and updating DTA entities...\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "registry_records = []\n",
    "\n",
    "for dta_id, major_version in major_versions.items():\n",
    "    meta = dta_metadata.get(dta_id, {})\n",
    "    draft_version = draft_versions.get(dta_id)\n",
    "    lib_type_data = major_versions_by_dta.get(dta_id, {})\n",
    "    \n",
    "    print(f\"\\n  DTA: {meta.get('dta_number')}\")\n",
    "    \n",
    "    # Update draft registry entries to ARCHIVED with effective_end_ts (for all library types)\n",
    "    if draft_version:\n",
    "        spark.sql(f\"\"\"\n",
    "            UPDATE {registry_table}\n",
    "            SET status = 'ARCHIVED', \n",
    "                effective_end_ts = current_timestamp(),\n",
    "                last_updated_ts = current_timestamp()\n",
    "            WHERE version = '{draft_version}' AND dta_id = '{dta_id}'\n",
    "        \"\"\")\n",
    "        print(f\"    ✓ Marked draft versions as ARCHIVED\")\n",
    "    \n",
    "    # Register DTA_APPROVED for each library type with records\n",
    "    for library_type, type_info in lib_type_data.items():\n",
    "        gold_count = type_info.get(\"gold_count\", 0)\n",
    "        \n",
    "        if gold_count == 0:\n",
    "            print(f\"    [{library_type}] Skipped (0 records)\")\n",
    "            continue\n",
    "        \n",
    "        registry_record = {\n",
    "            \"version\": major_version,\n",
    "            \"library_type\": library_type,\n",
    "            \"version_type\": \"DTA_APPROVED\",\n",
    "            \"dta_id\": dta_id,\n",
    "            \"parent_version\": base_template_version,\n",
    "            \"record_count\": gold_count,\n",
    "            \"status\": \"ACTIVE\",\n",
    "            \"data_provider_name\": meta.get(\"data_provider_name\"),\n",
    "            \"data_stream_type\": meta.get(\"data_stream_type\"),\n",
    "            \"included_dta_ids\": None,  # Not applicable for DTA_APPROVED\n",
    "            \"created_by_principal\": created_by_principal,\n",
    "            \"created_ts\": now,\n",
    "            \"last_updated_by_principal\": created_by_principal,\n",
    "            \"last_updated_ts\": now,\n",
    "            \"databricks_job_id\": databricks_job_id,\n",
    "            \"databricks_job_name\": databricks_job_name,\n",
    "            \"databricks_run_id\": databricks_run_id\n",
    "        }\n",
    "        registry_records.append(registry_record)\n",
    "        print(f\"    [{library_type}] Registered: {major_version} ({gold_count} records)\")\n",
    "    \n",
    "    # Update DTA entity (once per DTA) - status aligns 1:1 with md_version_registry\n",
    "    spark.sql(f\"\"\"\n",
    "        UPDATE {dta_table}\n",
    "        SET version = '{major_version}',\n",
    "            status = 'ACTIVE',\n",
    "            workflow_state = 'APPROVED',\n",
    "            last_updated_ts = current_timestamp()\n",
    "        WHERE dta_id = '{dta_id}'\n",
    "    \"\"\")\n",
    "    print(f\"    ✓ Updated DTA entity\")\n",
    "    \n",
    "    # Log activity: DTA Major created\n",
    "    log_dta_version_event(\n",
    "        spark=spark,\n",
    "        catalog=catalog,\n",
    "        dta_id=dta_id,\n",
    "        activity_type=\"DTA_APPROVED_CREATED\",\n",
    "        version=major_version,\n",
    "        performed_by=created_by_principal,\n",
    "        parent_version=draft_version\n",
    "    )\n",
    "    \n",
    "    # Log workflow approval (for historical: auto-approved)\n",
    "    if source == \"HISTORICAL\":\n",
    "        log_dta_workflow_event(\n",
    "            spark=spark,\n",
    "            catalog=catalog,\n",
    "            dta_id=dta_id,\n",
    "            activity_type=\"APPROVED\",\n",
    "            performed_by=created_by_principal,\n",
    "            workflow_iteration=1,\n",
    "            approver_role=\"SYSTEM\",\n",
    "            approver_name=\"Historical Import\",\n",
    "            comment=\"Auto-approved from historical DTA import\"\n",
    "        )\n",
    "    \n",
    "    print(f\"    ✓ Activity logged\")\n",
    "\n",
    "# Save all registry records\n",
    "if registry_records:\n",
    "    registry_df = spark.createDataFrame(registry_records, schema=get_version_registry_schema())\n",
    "    registry_df.write.format(\"delta\").mode(\"append\").saveAsTable(registry_table)\n",
    "    print(f\"\\n✓ Registered {len(registry_records)} DTA_APPROVED version(s) in registry\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Set Task Values and Summary\n",
    "dbutils.jobs.taskValues.set(key=\"major_versions\", value=json.dumps(major_versions))\n",
    "dbutils.jobs.taskValues.set(key=\"major_versions_by_dta\", value=json.dumps(major_versions_by_dta))\n",
    "dbutils.jobs.taskValues.set(key=\"dta_ids\", value=json.dumps(dta_ids))\n",
    "dbutils.jobs.taskValues.set(key=\"action\", value=\"APPROVE_DTA\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARY\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Source: {source}\")\n",
    "print(f\"Library Types: {list(library_table_map.keys())}\")\n",
    "print(f\"DTAs approved: {len(major_versions)}\")\n",
    "\n",
    "for dta_id, version in major_versions.items():\n",
    "    meta = dta_metadata.get(dta_id, {})\n",
    "    lib_type_data = major_versions_by_dta.get(dta_id, {})\n",
    "    print(f\"\\n  {meta.get('dta_number')}: {version}\")\n",
    "    for lib_type, type_info in lib_type_data.items():\n",
    "        print(f\"    - {lib_type}: {type_info.get('gold_count', 0)} records\")\n",
    "\n",
    "print(f\"\\n✅ DTA Major versions created successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
