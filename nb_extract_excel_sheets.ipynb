{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excel Sheet Metadata Extraction - tsDTA Processor\n",
    "\n",
    "**Purpose**: Extract sheet metadata (names, indices, categories) from tsDTA Excel documents stored in md_file_history table.\n",
    "\n",
    "**Input**: md_file_history table (filtered for tsDTA Excel documents with READY_FOR_PROCESSING status)\n",
    "\n",
    "**Output**: md_dta_excel_file_raw table (one row per sheet - metadata only, NO data content)\n",
    "\n",
    "**Processing**: Scalable parallel processing using mapInPandas for millions of documents\n",
    "\n",
    "**Filters Applied**:\n",
    "- Tags contain \"tsDTA\"\n",
    "- Active = true\n",
    "- Current status = \"READY_FOR_PROCESSING\"\n",
    "- File extension matches Excel formats (.xls, .xlsx, .xlsm, .xlsb, .xltx, .xltm)\n",
    "\n",
    "**Optimizations**: \n",
    "- mapInPandas for distributed parallel processing across Spark workers\n",
    "- Z-ORDER by (document_id, sheet_category) for fast queries\n",
    "- Conditional OPTIMIZE to prevent small file accumulation\n",
    "- Per-document status tracking for lineage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import os\n",
    "import json\n",
    "import base64\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lit, current_timestamp\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "from clinical_data_standards_framework.utils import (\n",
    "    get_excel_sheets_schema,\n",
    "    \n",
    "    categorize_sheet_name,\n",
    "    extract_version_history_metadata,\n",
    "    save_with_audit\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Get Configuration from Setup Task\n",
    "globals_dict = json.loads(dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"globals\"))\n",
    "pipeline_config = json.loads(dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"pipeline_config\"))\n",
    "created_by_principal = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"created_by_principal\")\n",
    "databricks_job_id = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"databricks_job_id\")\n",
    "databricks_job_name = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"databricks_job_name\")\n",
    "databricks_run_id = dbutils.jobs.taskValues.get(taskKey=\"setup\", key=\"databricks_run_id\")\n",
    "\n",
    "# Extract needed values from globals\n",
    "catalog = globals_dict['catalog']\n",
    "schema = globals_dict['bronze_schema']\n",
    "\n",
    "print(f\"Catalog: {catalog}, Schema: {schema}\")\n",
    "print(f\"Created by: {created_by_principal}\")\n",
    "print(f\"Job ID: {databricks_job_id}, Job Name: {databricks_job_name}, Run ID: {databricks_run_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Extract Pipeline-Specific Config\n",
    "source_config = pipeline_config['xls']['standard']['source']\n",
    "output_config = pipeline_config['xls']['standard']['output']\n",
    "\n",
    "source_table = source_config['source_table']\n",
    "filter_tags = source_config['filter_tags']\n",
    "filter_active = source_config['filter_active']\n",
    "filter_status = source_config['filter_status']\n",
    "excel_sheets_table = output_config['excel_sheets_table']\n",
    "excel_sheets_table_description = output_config.get('excel_sheets_table_description', '')\n",
    "\n",
    "# Construct full table names\n",
    "source_table_full_name = f\"{catalog}.{schema}.{source_table}\"\n",
    "excel_sheets_table_full_name = f\"{catalog}.{schema}.{excel_sheets_table}\"\n",
    "print(f\"Source table: {source_table_full_name}\")\n",
    "print(f\"Output table: {excel_sheets_table_full_name}\")\n",
    "print(f\"Filter: tags={filter_tags}, active={filter_active}, status={filter_status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Read Documents - BATCH MODE\n",
    "# Read from md_file_history table (batch read, not streaming)\n",
    "# This avoids circular dependencies when updating the same table\n",
    "df_source = spark.read.format(\"delta\").table(source_table_full_name)\n",
    "\n",
    "# Filter for tsDTA Excel documents that are ready for processing\n",
    "# File pattern: *.xls* matches .xls, .xlsx, .xlsm, .xlsb, .xltx, .xltm\n",
    "# Only process current versions (prevents duplicate processing when files are re-uploaded)\n",
    "df_filtered = df_source.filter(\n",
    "    (F.array_contains(F.col(\"document_tags\"), filter_tags[0])) &\n",
    "    (F.col(\"active\") == filter_active) &\n",
    "    (F.col(\"status\") == filter_status) &\n",
    "    (F.col(\"is_current\") == True) &  # â† Only process current version\n",
    "    (F.col(\"file_extension\").rlike(\"^\\\\.(xls|xlsx|xlsm|xlsb|xltx|xltm)$\"))\n",
    ")\n",
    "\n",
    "# Count documents to process\n",
    "doc_count = df_filtered.count()\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Documents to process: {doc_count}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "if doc_count == 0:\n",
    "    print(\"âš  No documents to process.\")\n",
    "    print(f\"Checking source table {source_table_full_name}...\")\n",
    "    \n",
    "    total_count = df_source.count()\n",
    "    print(f\"  Total records in source: {total_count}\")\n",
    "    \n",
    "    tag_count = df_source.filter(F.array_contains(F.col(\"document_tags\"), filter_tags[0])).count()\n",
    "    print(f\"  Records with tag '{filter_tags[0]}': {tag_count}\")\n",
    "    \n",
    "    active_count = df_source.filter(F.col(\"active\") == filter_active).count()\n",
    "    print(f\"  Records with active={filter_active}: {active_count}\")\n",
    "    \n",
    "    status_count = df_source.filter(F.col(\"status\") == filter_status).count()\n",
    "    print(f\"  Records with status='{filter_status}': {status_count}\")\n",
    "    \n",
    "    current_count = df_source.filter(F.col(\"is_current\") == True).count()\n",
    "    print(f\"  Records with is_current=true: {current_count}\")\n",
    "    \n",
    "    excel_count = df_source.filter(F.col(\"file_extension\").rlike(\"^\\\\.(xls|xlsx|xlsm|xlsb|xltx|xltm)$\")).count()\n",
    "    print(f\"  Records with Excel extensions: {excel_count}\")\n",
    "    \n",
    "    # Update status for documents that have the tag but failed filter criteria\n",
    "    # This helps identify misconfigured uploads (e.g., active=false)\n",
    "    docs_with_tag_not_active = df_source.filter(\n",
    "        (F.array_contains(F.col(\"document_tags\"), filter_tags[0])) &\n",
    "        (F.col(\"status\") == filter_status) &\n",
    "        (F.col(\"is_current\") == True) &\n",
    "        (F.col(\"active\") == False)\n",
    "    ).count()\n",
    "    \n",
    "    if docs_with_tag_not_active > 0:\n",
    "        print(f\"\\nâš ï¸  Found {docs_with_tag_not_active} documents with tag '{filter_tags[0]}' but active=false\")\n",
    "        print(\"   These documents were likely uploaded with incorrect settings.\")\n",
    "        \n",
    "        # Update these documents with a note explaining why they weren't processed\n",
    "        spark.sql(f\"\"\"\n",
    "            UPDATE {source_table_full_name}\n",
    "            SET status = 'XLS_SKIPPED',\n",
    "                notes = CONCAT(COALESCE(notes, ''), ' | Skipped: active=false, document not eligible for processing'),\n",
    "                last_updated_ts = current_timestamp()\n",
    "            WHERE array_contains(document_tags, '{filter_tags[0]}')\n",
    "              AND status = '{filter_status}'\n",
    "              AND is_current = true\n",
    "              AND active = false\n",
    "        \"\"\")\n",
    "        print(f\"   Updated {docs_with_tag_not_active} documents with status='XLS_SKIPPED'\")\n",
    "    \n",
    "    # Set task value to signal downstream tasks to skip\n",
    "    dbutils.jobs.taskValues.set(key=\"documents_found\", value=\"false\")\n",
    "    dbutils.jobs.taskValues.set(key=\"document_count\", value=\"0\")\n",
    "    \n",
    "    print(\"\\nâ­ï¸  Setting task value 'documents_found' = false\")\n",
    "    print(\"   Downstream tasks will be skipped.\")\n",
    "    dbutils.notebook.exit(\"No documents to process - downstream tasks will be skipped\")\n",
    "\n",
    "# Set task value indicating documents were found\n",
    "dbutils.jobs.taskValues.set(key=\"documents_found\", value=\"true\")\n",
    "dbutils.jobs.taskValues.set(key=\"document_count\", value=str(doc_count))\n",
    "print(f\"\\nâœ“ Setting task value 'documents_found' = true\")\n",
    "print(f\"  Document count: {doc_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Process Excel Files - mapInPandas for Scalable Parallel Processing\n",
    "# This approach distributes processing across Spark workers for millions of documents\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType\n",
    "\n",
    "# Define result schema for mapInPandas output\n",
    "# This includes both sheet records and processing status per document\n",
    "# Note: DoubleType used for processing_duration_seconds to match ()\n",
    "result_schema = StructType([\n",
    "    StructField(\"record_type\", StringType(), False),  # 'sheet' or 'status' or 'update'\n",
    "    StructField(\"document_id\", StringType(), False),\n",
    "    StructField(\"parent_document_id\", StringType(), True),\n",
    "    StructField(\"source_file_path\", StringType(), True),\n",
    "    StructField(\"sheet_name\", StringType(), True),\n",
    "    StructField(\"sheet_index\", IntegerType(), True),\n",
    "    StructField(\"sheet_category\", StringType(), True),\n",
    "    StructField(\"trial_id\", StringType(), True),\n",
    "    StructField(\"data_stream_type\", StringType(), True),\n",
    "    StructField(\"data_provider_name\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"status_timestamp\", TimestampType(), True),\n",
    "    StructField(\"error_message\", StringType(), True),\n",
    "    StructField(\"processing_duration_seconds\", DoubleType(), True),\n",
    "    StructField(\"processing_metadata\", StringType(), True),\n",
    "    StructField(\"new_status\", StringType(), True),\n",
    "    StructField(\"new_status_timestamp\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "def process_excel_pandas(pdf_iterator):\n",
    "    \"\"\"\n",
    "    Process Excel documents using mapInPandas.\n",
    "    Each partition is processed in parallel across Spark workers.\n",
    "    Yields sheet records, status records, and update records for each document.\n",
    "    \n",
    "    Parent Document ID Logic:\n",
    "    - For files extracted from a ZIP: use fh_parent_document_id (the ZIP's document_id)\n",
    "    - For standalone uploads: fh_parent_document_id is NULL, use the file's document_id as fallback\n",
    "    \"\"\"\n",
    "    for pdf in pdf_iterator:\n",
    "        results = []\n",
    "        \n",
    "        for _, row in pdf.iterrows():\n",
    "            document_id = row['document_id']\n",
    "            extracted_path = row['extracted_path']\n",
    "            content_b64 = row['content_base64']\n",
    "            # Get parent_document_id from md_file_history\n",
    "            # For ZIP extracts: this is the ZIP's document_id\n",
    "            # For standalone uploads: this is None/NaN\n",
    "            fh_parent_document_id = row.get('fh_parent_document_id')\n",
    "            \n",
    "            # Determine the correct parent_document_id:\n",
    "            # - If file came from ZIP: use the ZIP's document_id (fh_parent_document_id)\n",
    "            # - If standalone file: use the file's own document_id as the parent\n",
    "            if pd.notna(fh_parent_document_id) and fh_parent_document_id:\n",
    "                actual_parent_doc_id = str(fh_parent_document_id)\n",
    "            else:\n",
    "                actual_parent_doc_id = document_id\n",
    "            \n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            try:\n",
    "                # Decode base64 content\n",
    "                content_binary = base64.b64decode(content_b64)\n",
    "                excel_file = BytesIO(content_binary)\n",
    "                \n",
    "                # Detect file extension and choose correct engine\n",
    "                file_ext = os.path.splitext(extracted_path.lower())[1]\n",
    "                \n",
    "                if file_ext in ['.xls', '.xlt']:\n",
    "                    xl = pd.ExcelFile(excel_file, engine='xlrd')\n",
    "                else:\n",
    "                    xl = pd.ExcelFile(excel_file, engine='openpyxl')\n",
    "                \n",
    "                # Extract Version History metadata once per Excel file\n",
    "                version_metadata = extract_version_history_metadata(content_binary, file_ext)\n",
    "                trial_id = version_metadata.get(\"trial_id\", \"UNKNOWN\")\n",
    "                data_stream_type = version_metadata.get(\"data_stream_type\", \"UNKNOWN\")\n",
    "                data_provider_name = version_metadata.get(\"data_provider_name\", \"UNKNOWN\")\n",
    "                \n",
    "                # Extract each sheet metadata with category\n",
    "                for sheet_idx, sheet_name in enumerate(xl.sheet_names):\n",
    "                    sheet_category = categorize_sheet_name(sheet_name)\n",
    "                    \n",
    "                    results.append({\n",
    "                        'record_type': 'sheet',\n",
    "                        'document_id': str(uuid.uuid4()),  # Unique sheet ID\n",
    "                        'parent_document_id': actual_parent_doc_id,  # Use correct parent (ZIP or self)\n",
    "                        'source_file_path': extracted_path,\n",
    "                        'sheet_name': sheet_name,\n",
    "                        'sheet_index': sheet_idx,\n",
    "                        'sheet_category': sheet_category,\n",
    "                        'trial_id': trial_id,\n",
    "                        'data_stream_type': data_stream_type,\n",
    "                        'data_provider_name': data_provider_name,\n",
    "                        'status': None,\n",
    "                        'status_timestamp': None,\n",
    "                        'error_message': None,\n",
    "                        'processing_duration_seconds': None,\n",
    "                        'processing_metadata': None,\n",
    "                        'new_status': None,\n",
    "                        'new_status_timestamp': None\n",
    "                    })\n",
    "                \n",
    "                # Calculate duration\n",
    "                end_time = datetime.now()\n",
    "                duration = (end_time - start_time).total_seconds()\n",
    "                \n",
    "                # Status record: COMPLETED\n",
    "                results.append({\n",
    "                    'record_type': 'status',\n",
    "                    'document_id': document_id,\n",
    "                    'parent_document_id': None,\n",
    "                    'source_file_path': None,\n",
    "                    'sheet_name': None,\n",
    "                    'sheet_index': None,\n",
    "                    'sheet_category': None,\n",
    "                    'trial_id': None,\n",
    "                    'data_stream_type': None,\n",
    "                    'data_provider_name': None,\n",
    "                    'status': 'EXCEL_EXTRACTION_COMPLETED',\n",
    "                    'status_timestamp': end_time,\n",
    "                    'error_message': None,\n",
    "                    'processing_duration_seconds': duration,\n",
    "                    'processing_metadata': json.dumps({\n",
    "                        'sheets_extracted': len(xl.sheet_names),\n",
    "                        'source_path': extracted_path\n",
    "                    }),\n",
    "                    'new_status': None,\n",
    "                    'new_status_timestamp': None\n",
    "                })\n",
    "                \n",
    "                # Update record for manifest\n",
    "                results.append({\n",
    "                    'record_type': 'update',\n",
    "                    'document_id': document_id,\n",
    "                    'parent_document_id': None,\n",
    "                    'source_file_path': None,\n",
    "                    'sheet_name': None,\n",
    "                    'sheet_index': None,\n",
    "                    'sheet_category': None,\n",
    "                    'trial_id': None,\n",
    "                    'data_stream_type': None,\n",
    "                    'data_provider_name': None,\n",
    "                    'status': None,\n",
    "                    'status_timestamp': None,\n",
    "                    'error_message': None,\n",
    "                    'processing_duration_seconds': None,\n",
    "                    'processing_metadata': None,\n",
    "                    'new_status': 'EXCEL_EXTRACTION_COMPLETED',\n",
    "                    'new_status_timestamp': end_time\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Error handling - create error records\n",
    "                error_time = datetime.now()\n",
    "                duration = (error_time - start_time).total_seconds()\n",
    "                \n",
    "                # Status record: ERROR\n",
    "                results.append({\n",
    "                    'record_type': 'status',\n",
    "                    'document_id': document_id,\n",
    "                    'parent_document_id': None,\n",
    "                    'source_file_path': None,\n",
    "                    'sheet_name': None,\n",
    "                    'sheet_index': None,\n",
    "                    'sheet_category': None,\n",
    "                    'trial_id': None,\n",
    "                    'data_stream_type': None,\n",
    "                    'data_provider_name': None,\n",
    "                    'status': 'EXCEL_EXTRACTION_ERROR',\n",
    "                    'status_timestamp': error_time,\n",
    "                    'error_message': str(e)[:1000],  # Truncate long errors\n",
    "                    'processing_duration_seconds': duration,\n",
    "                    'processing_metadata': json.dumps({'source_path': extracted_path}),\n",
    "                    'new_status': None,\n",
    "                    'new_status_timestamp': None\n",
    "                })\n",
    "                \n",
    "                # Update record for manifest\n",
    "                results.append({\n",
    "                    'record_type': 'update',\n",
    "                    'document_id': document_id,\n",
    "                    'parent_document_id': None,\n",
    "                    'source_file_path': None,\n",
    "                    'sheet_name': None,\n",
    "                    'sheet_index': None,\n",
    "                    'sheet_category': None,\n",
    "                    'trial_id': None,\n",
    "                    'data_stream_type': None,\n",
    "                    'data_provider_name': None,\n",
    "                    'status': None,\n",
    "                    'status_timestamp': None,\n",
    "                    'error_message': None,\n",
    "                    'processing_duration_seconds': None,\n",
    "                    'processing_metadata': None,\n",
    "                    'new_status': 'EXCEL_EXTRACTION_ERROR',\n",
    "                    'new_status_timestamp': error_time\n",
    "                })\n",
    "        \n",
    "        # Yield results as DataFrame\n",
    "        if results:\n",
    "            yield pd.DataFrame(results)\n",
    "        else:\n",
    "            # Yield empty DataFrame with correct schema\n",
    "            yield pd.DataFrame(columns=[f.name for f in result_schema.fields])\n",
    "\n",
    "print(f\"âœ“ mapInPandas processing function defined\")\n",
    "print(f\"  - Parallel processing across Spark workers\")\n",
    "print(f\"  - Per-document status tracking for lineage\")\n",
    "print(f\"  - Error handling with detailed messages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Execute mapInPandas and Write Results\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Starting parallel processing with mapInPandas...\")\n",
    "print(f\"Documents to process: {doc_count}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Repartition for optimal parallelism (1 partition per ~10 docs, min 1, max 200)\n",
    "num_partitions = min(max(1, doc_count // 10), 200)\n",
    "# Include parent_document_id from md_file_history to correctly link sheets to ZIP parent\n",
    "# Rename to fh_parent_document_id to distinguish from the sheet's parent_document_id\n",
    "df_repartitioned = df_filtered.select(\n",
    "    'document_id',\n",
    "    F.col('parent_document_id').alias('fh_parent_document_id'),  # From file history (ZIP's doc_id or NULL)\n",
    "    'extracted_path',\n",
    "    'content_base64'\n",
    ").repartition(num_partitions)\n",
    "\n",
    "print(f\"Using {num_partitions} partitions for parallel processing\")\n",
    "print(f\"  Including fh_parent_document_id for ZIP file lineage tracking\")\n",
    "\n",
    "# Execute mapInPandas - this runs in parallel across Spark workers\n",
    "df_results = df_repartitioned.mapInPandas(process_excel_pandas, result_schema)\n",
    "\n",
    "# Note: cache() not supported on serverless compute - Spark will recompute as needed\n",
    "\n",
    "# Split results by record_type\n",
    "df_sheets = df_results.filter(F.col(\"record_type\") == \"sheet\").select(\n",
    "    \"document_id\", \"parent_document_id\", \"source_file_path\",\n",
    "    \"sheet_name\", \"sheet_index\", \"sheet_category\",\n",
    "    \"trial_id\", \"data_stream_type\", \"data_provider_name\"\n",
    ")\n",
    "\n",
    "df_status = df_results.filter(F.col(\"record_type\") == \"status\").select(\n",
    "    \"document_id\", \"status\", \"status_timestamp\",\n",
    "    \"error_message\", \"processing_duration_seconds\", \"processing_metadata\"\n",
    ").withColumn(\"retry_count\", lit(0))\n",
    "\n",
    "df_updates = df_results.filter(F.col(\"record_type\") == \"update\").select(\n",
    "    \"document_id\", \"new_status\", \"new_status_timestamp\"\n",
    ")\n",
    "\n",
    "# Count results\n",
    "sheet_count = df_sheets.count()\n",
    "success_count = df_status.filter(F.col(\"status\") == \"EXCEL_EXTRACTION_COMPLETED\").count()\n",
    "error_count = df_status.filter(F.col(\"status\") == \"EXCEL_EXTRACTION_ERROR\").count()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Processing Results:\")\n",
    "print(f\"  Sheets extracted: {sheet_count}\")\n",
    "print(f\"  Documents SUCCESS: {success_count}\")\n",
    "print(f\"  Documents ERROR: {error_count}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Write sheet records\n",
    "if sheet_count > 0:\n",
    "    print(f'\\nSaving {sheet_count} sheets to {excel_sheets_table_full_name}...')\n",
    "    save_with_audit(\n",
    "        df=df_sheets,\n",
    "        table_name=excel_sheets_table_full_name,\n",
    "        created_by_principal=created_by_principal,\n",
    "        databricks_job_id=databricks_job_id,\n",
    "        databricks_job_name=databricks_job_name,\n",
    "        databricks_run_id=databricks_run_id,\n",
    "        mode='append'\n",
    "    )\n",
    "    print(f'âœ“ Saved {sheet_count} sheets with audit columns')\n",
    "\n",
    "# Update md_file_history table with processing status via MERGE\n",
    "# Status is now consolidated in md_file_history - no separate status table needed\n",
    "update_count = df_updates.count()\n",
    "if update_count > 0:\n",
    "    print(f'\\nUpdating {update_count} documents in manifest...')\n",
    "    delta_table = DeltaTable.forName(spark, source_table_full_name)\n",
    "    \n",
    "    delta_table.alias(\"target\").merge(\n",
    "        df_updates.alias(\"updates\"),\n",
    "        \"target.document_id = updates.document_id\"\n",
    "    ).whenMatchedUpdate(set={\n",
    "        \"status\": \"updates.new_status\",\n",
    "        \"status_timestamp\": \"updates.new_status_timestamp\",\n",
    "        \"last_updated_by_principal\": lit(created_by_principal),\n",
    "        \"last_updated_ts\": current_timestamp(),\n",
    "        \"databricks_job_id\": lit(databricks_job_id),\n",
    "        \"databricks_job_name\": lit(databricks_job_name),\n",
    "        \"databricks_run_id\": lit(databricks_run_id)\n",
    "    }).execute()\n",
    "    \n",
    "    print(f'âœ“ Updated {update_count} documents')\n",
    "\n",
    "# Set task values for downstream tasks\n",
    "dbutils.jobs.taskValues.set(key=\"excel_extraction_status\", value=\"SUCCESS\" if success_count > 0 else \"FAILED\")\n",
    "dbutils.jobs.taskValues.set(key=\"excel_sheets_count\", value=str(sheet_count))\n",
    "dbutils.jobs.taskValues.set(key=\"excel_success_count\", value=str(success_count))\n",
    "dbutils.jobs.taskValues.set(key=\"excel_error_count\", value=str(error_count))\n",
    "\n",
    "# Set parent_document_id for downstream tasks (required for test concepts, codelists, etc.)\n",
    "if sheet_count > 0:\n",
    "    first_parent_doc = df_sheets.select(\"parent_document_id\").first()\n",
    "    if first_parent_doc and first_parent_doc['parent_document_id']:\n",
    "        dbutils.jobs.taskValues.set(key=\"parent_document_id\", value=first_parent_doc['parent_document_id'])\n",
    "        print(f\"  Set parent_document_id: {first_parent_doc['parent_document_id']}\")\n",
    "    else:\n",
    "        print(\"  âš  No parent_document_id found in sheets\")\n",
    "else:\n",
    "    print(\"  âš  No sheets processed - parent_document_id not set\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"âœ“ Parallel processing completed!\")\n",
    "print(f\"{'='*80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Conditional OPTIMIZE - md_dta_excel_file_raw table\n",
    "# Only run OPTIMIZE when conditions are met to save compute resources\n",
    "\n",
    "# Get table stats\n",
    "table_stats = spark.sql(f\"DESCRIBE DETAIL {excel_sheets_table_full_name}\").first()\n",
    "num_files = table_stats.numFiles\n",
    "total_size_gb = table_stats.sizeInBytes / (1024**3)\n",
    "avg_file_size_mb = (total_size_gb * 1024) / num_files if num_files > 0 else 0\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"TABLE STATS: {excel_sheets_table_full_name}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Sheets processed: {doc_count}\")\n",
    "print(f\"Total files: {num_files}\")\n",
    "print(f\"Total size: {total_size_gb:.2f} GB\")\n",
    "print(f\"Avg file size: {avg_file_size_mb:.2f} MB\")\n",
    "\n",
    "# Determine if OPTIMIZE is needed\n",
    "should_optimize = False\n",
    "reason = \"\"\n",
    "\n",
    "if doc_count > 100:  # Condition 1: Large batch\n",
    "    should_optimize = True\n",
    "    reason = f\"Large batch processed ({doc_count} documents)\"\n",
    "elif num_files > 200 and avg_file_size_mb < 100:  # Condition 2: Small file problem\n",
    "    should_optimize = True\n",
    "    reason = f\"Small file problem ({num_files} files, avg {avg_file_size_mb:.2f}MB)\"\n",
    "elif num_files > 500:  # Condition 3: Too many files\n",
    "    should_optimize = True\n",
    "    reason = f\"Too many files ({num_files})\"\n",
    "\n",
    "if should_optimize:\n",
    "    print(f\"\\nðŸ”§ OPTIMIZE TRIGGERED: {reason}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Run OPTIMIZE with Z-ORDER\n",
    "    spark.sql(f\"\"\"\n",
    "        OPTIMIZE {excel_sheets_table_full_name}\n",
    "        ZORDER BY (document_id, sheet_category)\n",
    "    \"\"\")\n",
    "    \n",
    "    # Get stats after optimization\n",
    "    table_stats_after = spark.sql(f\"DESCRIBE DETAIL {excel_sheets_table_full_name}\").first()\n",
    "    num_files_after = table_stats_after.numFiles\n",
    "    \n",
    "    duration = (datetime.now() - start_time).total_seconds()\n",
    "    \n",
    "    print(f\"âœ“ OPTIMIZE completed in {duration:.1f} seconds\")\n",
    "    print(f\"  Files: {num_files} â†’ {num_files_after} ({num_files - num_files_after} merged)\")\n",
    "    print(f\"{'='*80}\")\n",
    "else:\n",
    "    print(f\"\\nâ­ï¸  OPTIMIZE SKIPPED: Conditions not met\")\n",
    "    print(f\"  - Docs processed: {doc_count} (threshold: >100)\")\n",
    "    print(f\"  - Files: {num_files} (threshold: >200 with small files OR >500 total)\")\n",
    "    print(f\"  - Avg file size: {avg_file_size_mb:.2f}MB (threshold: <100MB)\")\n",
    "    print(f\"{'='*80}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "generate_dta_file_metadata",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
